
### Types of Ensemble Methods

#### 1. Bagging (Bootstrap Aggregating)
**Strategy:** Train multiple models on different subsets of data

| Method | Description | Pros | Cons |
|--------|-------------|------|------|
| **Random Forest** | Bagging + Random features | Reduces overfitting, fast | Less interpretable |
| **Extra Trees** | Extremely randomized trees | Even faster training | Higher variance |

**How it works:**
```
Dataset ‚Üí Bootstrap Sample 1 ‚Üí Model 1 ‚îê
        ‚Üí Bootstrap Sample 2 ‚Üí Model 2 ‚îú‚Üí Average/Vote ‚Üí Final Prediction
        ‚Üí Bootstrap Sample n ‚Üí Model n ‚îò
```

#### 2. Boosting
**Strategy:** Train models sequentially, each correcting previous errors

| Method | Description | Pros | Cons |
|--------|-------------|------|------|
| **AdaBoost** | Adaptive boosting | Good performance, interpretable | Sensitive to noise |
| **Gradient Boosting** | Fits residuals iteratively | Excellent performance | Prone to overfitting |
| **XGBoost** | Optimized gradient boosting | State-of-the-art performance | Many hyperparameters |
| **LightGBM** | Fast gradient boosting | Very fast, efficient | Requires tuning |
| **CatBoost** | Handles categorical features | Automatic cat. handling | Less mature |

**How it works:**
```
Model 1 ‚Üí Errors 1 ‚Üí Model 2 ‚Üí Errors 2 ‚Üí Model 3 ‚Üí ... ‚Üí Final Model
```

#### 3. Stacking
**Strategy:** Use a meta-model to combine predictions from base models

**How it works:**
```
Base Model 1 ‚îê
Base Model 2 ‚îú‚Üí Meta-Model ‚Üí Final Prediction
Base Model 3 ‚îò
```

#### 4. Voting
**Strategy:** Combine predictions through voting or averaging

- **Hard Voting**: Majority vote for classification
- **Soft Voting**: Average predicted probabilities


## üìä Performance Comparison

### Ensemble vs Individual Models
```
Individual Model:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 80% accuracy
Random Forest:        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 85% accuracy  (+5%)
Gradient Boosting:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 88% accuracy (+8%)
Stacked Ensemble:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 90% accuracy (+10%)
```