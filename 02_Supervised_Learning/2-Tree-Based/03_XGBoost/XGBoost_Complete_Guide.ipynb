{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda21746",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> <strong>üöÄ XGBoost - Complete Guide</strong> </h1>\n",
    "\n",
    "This notebook provides a comprehensive introduction to XGBoost (eXtreme Gradient Boosting), covering:\n",
    "- Conceptual foundation and gradient boosting\n",
    "- Implementation with XGBoost library\n",
    "- Model evaluation and interpretation\n",
    "- Hyperparameter tuning and optimization\n",
    "- Comparison with Random Forest and Decision Trees\n",
    "- Advanced features and techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a3d54",
   "metadata": {},
   "source": [
    "## **üìö 1. Import Libraries and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a940290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                           mean_squared_error, r2_score, mean_absolute_error,\n",
    "                           precision_score, recall_score, f1_score, log_loss)\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_wine, load_breast_cancer\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1d446",
   "metadata": {},
   "source": [
    "## **üß† 2. Conceptual Foundation**\n",
    "\n",
    "### **What is XGBoost?** ü§î\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized gradient boosting framework designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework.\n",
    "\n",
    "### **Key Concepts:**\n",
    "\n",
    "1. **Gradient Boosting**: Sequentially builds models, each correcting errors of previous ones\n",
    "2. **Regularization**: L1 and L2 regularization to prevent overfitting\n",
    "3. **Tree Pruning**: Intelligent pruning to avoid unnecessary splits\n",
    "4. **Parallel Processing**: Optimized for speed and performance\n",
    "5. **Handling Missing Values**: Built-in handling of missing data\n",
    "\n",
    "### **How XGBoost Works:**\n",
    "\n",
    "1. **Initialize**: Start with a simple prediction (e.g., mean)\n",
    "2. **Calculate Residuals**: Find errors from current prediction\n",
    "3. **Build Tree**: Train a tree to predict residuals\n",
    "4. **Update Model**: Add new tree with learning rate\n",
    "5. **Repeat**: Continue until convergence or max iterations\n",
    "6. **Final Prediction**: Sum of all tree predictions\n",
    "\n",
    "### **Mathematical Foundation:**\n",
    "\n",
    "#### Objective Function:\n",
    "$$\\text{Obj}^{(t)} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^{t} \\Omega(f_i)$$\n",
    "\n",
    "Where:\n",
    "- $l$ is the loss function\n",
    "- $\\Omega(f_i)$ is the regularization term for tree $f_i$\n",
    "\n",
    "#### Regularization:\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2$$\n",
    "\n",
    "Where:\n",
    "- $T$ = number of leaves\n",
    "- $w_j$ = leaf weights\n",
    "- $\\gamma$ = complexity penalty\n",
    "- $\\lambda$ = L2 regularization\n",
    "\n",
    "#### Learning Rate:\n",
    "$$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta f_t(x_i)$$\n",
    "\n",
    "Where $\\eta$ is the learning rate (shrinkage parameter).\n",
    "\n",
    "### **XGBoost vs Other Methods:**\n",
    "\n",
    "| Feature | Decision Tree | Random Forest | XGBoost |\n",
    "|---------|---------------|---------------|---------|\n",
    "| **Method** | Single tree | Parallel trees | Sequential trees |\n",
    "| **Bias** | High variance | Lower variance | Low bias & variance |\n",
    "| **Speed** | Fast | Medium | Medium-Fast |\n",
    "| **Accuracy** | Good | Better | Best |\n",
    "| **Overfitting** | High risk | Low risk | Controlled |\n",
    "| **Interpretability** | High | Medium | Medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9a1a2",
   "metadata": {},
   "source": [
    "## **üìä 3. Generate Sample Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Classification Dataset - More complex for gradient boosting\n",
    "print(\"üéØ Creating Classification Dataset\")\n",
    "X_cls, y_cls = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=15,\n",
    "    n_informative=10,\n",
    "    n_redundant=3,\n",
    "    n_clusters_per_class=2,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add some noise and missing values to make it more realistic\n",
    "noise_indices = np.random.choice(len(X_cls), size=int(0.05 * len(X_cls)), replace=False)\n",
    "X_cls[noise_indices] += np.random.normal(0, 2, size=(len(noise_indices), X_cls.shape[1]))\n",
    "\n",
    "feature_names_cls = [f'feature_{i}' for i in range(X_cls.shape[1])]\n",
    "df_cls = pd.DataFrame(X_cls, columns=feature_names_cls)\n",
    "df_cls['target'] = y_cls\n",
    "\n",
    "print(f\"Classification dataset shape: {df_cls.shape}\")\n",
    "print(f\"Classes: {np.unique(y_cls)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_cls)}\")\n",
    "\n",
    "# Regression Dataset - Non-linear relationship\n",
    "print(\"\\nüìà Creating Regression Dataset\")\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1500,\n",
    "    n_features=12,\n",
    "    n_informative=8,\n",
    "    noise=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add non-linear transformations\n",
    "X_reg[:, 0] = X_reg[:, 0] ** 2  # Quadratic relationship\n",
    "X_reg[:, 1] = np.sin(X_reg[:, 1])  # Sinusoidal relationship\n",
    "\n",
    "feature_names_reg = [f'feature_{i}' for i in range(X_reg.shape[1])]\n",
    "df_reg = pd.DataFrame(X_reg, columns=feature_names_reg)\n",
    "df_reg['target'] = y_reg\n",
    "\n",
    "print(f\"Regression dataset shape: {df_reg.shape}\")\n",
    "\n",
    "# Load Wine dataset for detailed analysis\n",
    "print(\"\\nüç∑ Loading Wine Dataset\")\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "print(f\"Wine dataset shape: {X_wine.shape}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "\n",
    "# Visualize relationships\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(df_cls['feature_0'], df_cls['feature_1'], c=y_cls, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.title('Classification Dataset')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(df_reg['feature_0'], df_reg['target'], alpha=0.6, color='orange')\n",
    "plt.xlabel('Feature 0 (Quadratic)')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Regression Dataset')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_wine[:, 0], X_wine[:, 1], c=y_wine, cmap='Set1', alpha=0.6)\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Malic Acid')\n",
    "plt.title('Wine Dataset')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6d111",
   "metadata": {},
   "source": [
    "## **üõ†Ô∏è 4. XGBoost Implementation - Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification with Wine dataset\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine\n",
    ")\n",
    "\n",
    "# Create XGBoost classifier\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss'  # For multiclass classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_clf.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = xgb_clf.predict(X_train_wine)\n",
    "y_test_pred = xgb_clf.predict(X_test_wine)\n",
    "y_test_proba = xgb_clf.predict_proba(X_test_wine)\n",
    "\n",
    "print(\"üöÄ XGBoost Classification Results:\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train_wine, y_train_pred):.3f}\")\n",
    "print(f\"Testing Accuracy:  {accuracy_score(y_test_wine, y_test_pred):.3f}\")\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test_wine, y_test_pred, average='weighted'):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test_wine, y_test_pred, average='weighted'):.3f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test_wine, y_test_pred, average='weighted'):.3f}\")\n",
    "\n",
    "# Compare with Random Forest and Gradient Boosting\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_clf.fit(X_train_wine, y_train_wine)\n",
    "gb_clf.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "rf_pred = rf_clf.predict(X_test_wine)\n",
    "gb_pred = gb_clf.predict(X_test_wine)\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest Accuracy:     {accuracy_score(y_test_wine, rf_pred):.3f}\")\n",
    "print(f\"üå≥ Gradient Boosting Accuracy: {accuracy_score(y_test_wine, gb_pred):.3f}\")\n",
    "print(f\"üöÄ XGBoost Accuracy:           {accuracy_score(y_test_wine, y_test_pred):.3f}\")\n",
    "\n",
    "# Feature importance comparison\n",
    "print(\"\\nüìä Top 10 Feature Importances (XGBoost):\")\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    'Feature': wine.feature_names,\n",
    "    'Importance': xgb_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(xgb_importance_df.head(10))\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test_wine, y_test_pred, target_names=wine.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb257c83",
   "metadata": {},
   "source": [
    "## **üìà 5. XGBoost Implementation - Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with synthetic dataset\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create XGBoost regressor\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_reg = xgb_reg.predict(X_train_reg)\n",
    "y_test_pred_reg = xgb_reg.predict(X_test_reg)\n",
    "\n",
    "print(\"üìà XGBoost Regression Results:\")\n",
    "print(f\"Training R¬≤ Score: {r2_score(y_train_reg, y_train_pred_reg):.3f}\")\n",
    "print(f\"Testing R¬≤ Score:  {r2_score(y_test_reg, y_test_pred_reg):.3f}\")\n",
    "\n",
    "print(f\"Training RMSE:     {np.sqrt(mean_squared_error(y_train_reg, y_train_pred_reg)):.3f}\")\n",
    "print(f\"Testing RMSE:      {np.sqrt(mean_squared_error(y_test_reg, y_test_pred_reg)):.3f}\")\n",
    "print(f\"Testing MAE:       {mean_absolute_error(y_test_reg, y_test_pred_reg):.3f}\")\n",
    "\n",
    "# Compare with other methods\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_reg.fit(X_train_reg, y_train_reg)\n",
    "gb_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "rf_pred_reg = rf_reg.predict(X_test_reg)\n",
    "gb_pred_reg = gb_reg.predict(X_test_reg)\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest R¬≤:     {r2_score(y_test_reg, rf_pred_reg):.3f}\")\n",
    "print(f\"üå≥ Gradient Boosting R¬≤: {r2_score(y_test_reg, gb_pred_reg):.3f}\")\n",
    "print(f\"üöÄ XGBoost R¬≤:           {r2_score(y_test_reg, y_test_pred_reg):.3f}\")\n",
    "\n",
    "print(\"\\nüìä Feature Importances (Regression):\")\n",
    "reg_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names_reg,\n",
    "    'Importance': xgb_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(reg_importance_df.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffd2cb",
   "metadata": {},
   "source": [
    "## **üìä 6. Advanced XGBoost Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XGBoost's native API for more control\n",
    "print(\"üîß XGBoost Native API Features\")\n",
    "\n",
    "# Convert to DMatrix for native XGBoost API\n",
    "dtrain = xgb.DMatrix(X_train_wine, label=y_train_wine)\n",
    "dtest = xgb.DMatrix(X_test_wine, label=y_test_wine)\n",
    "\n",
    "# Set parameters for native API\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,  # learning_rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train with evaluation set (early stopping)\n",
    "evallist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_round = 100\n",
    "\n",
    "xgb_native = xgb.train(\n",
    "    params, \n",
    "    dtrain, \n",
    "    num_round,\n",
    "    evallist,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {xgb_native.best_iteration}\")\n",
    "print(f\"Best score: {xgb_native.best_score:.4f}\")\n",
    "\n",
    "# Predict with native API\n",
    "y_pred_native = xgb_native.predict(dtest)\n",
    "y_pred_native_classes = np.argmax(y_pred_native, axis=1)\n",
    "\n",
    "print(f\"Native API Accuracy: {accuracy_score(y_test_wine, y_pred_native_classes):.3f}\")\n",
    "\n",
    "# Feature importance from native API\n",
    "importance_dict = xgb_native.get_importance()\n",
    "print(\"\\nüìä Feature Importance (Native API - by gain):\")\n",
    "for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{wine.feature_names[int(feature[1:])]: <20}: {importance:.3f}\")\n",
    "\n",
    "# Different importance types\n",
    "importance_types = ['weight', 'gain', 'cover']\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, imp_type in enumerate(importance_types):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    importance = xgb_native.get_importance(importance_type=imp_type)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    imp_df = pd.DataFrame([\n",
    "        {'feature': wine.feature_names[int(k[1:])], 'importance': v} \n",
    "        for k, v in importance.items()\n",
    "    ]).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.barh(imp_df['feature'], imp_df['importance'])\n",
    "    plt.title(f'Feature Importance ({imp_type})')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16deab32",
   "metadata": {},
   "source": [
    "## **üìä 7. Comprehensive Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# 1. Feature Importance Comparison\n",
    "top_features = xgb_importance_df.head(10)\n",
    "axes[0, 0].barh(top_features['Feature'], top_features['Importance'], color='lightgreen')\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "axes[0, 0].set_title('Top 10 Feature Importances')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(y_test_wine, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[0, 1],\n",
    "            xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "axes[0, 1].set_ylabel('Actual')\n",
    "axes[0, 1].set_title('Confusion Matrix')\n",
    "\n",
    "# 3. Learning Curves (using eval_set)\n",
    "xgb_eval = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "eval_set = [(X_train_wine, y_train_wine), (X_test_wine, y_test_wine)]\n",
    "xgb_eval.fit(X_train_wine, y_train_wine, \n",
    "             eval_set=eval_set, \n",
    "             verbose=False)\n",
    "\n",
    "# Get evaluation results\n",
    "results = xgb_eval.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "\n",
    "axes[0, 2].plot(range(epochs), results['validation_0']['mlogloss'], label='Train')\n",
    "axes[0, 2].plot(range(epochs), results['validation_1']['mlogloss'], label='Test')\n",
    "axes[0, 2].set_xlabel('Epochs')\n",
    "axes[0, 2].set_ylabel('Log Loss')\n",
    "axes[0, 2].set_title('Learning Curves')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model Comparison\n",
    "models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test_wine, rf_pred),\n",
    "    accuracy_score(y_test_wine, gb_pred),\n",
    "    accuracy_score(y_test_wine, y_test_pred)\n",
    "]\n",
    "\n",
    "bars = axes[1, 0].bar(models, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('Model Comparison')\n",
    "axes[1, 0].set_ylim([0.8, 1.0])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 5. Regression: True vs Predicted\n",
    "axes[1, 1].scatter(y_test_reg, y_test_pred_reg, alpha=0.6, color='green')\n",
    "axes[1, 1].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "                [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('True Values')\n",
    "axes[1, 1].set_ylabel('Predicted Values')\n",
    "axes[1, 1].set_title('Regression: True vs Predicted')\n",
    "\n",
    "# 6. Residuals Plot\n",
    "residuals = y_test_reg - y_test_pred_reg\n",
    "axes[1, 2].scatter(y_test_pred_reg, residuals, alpha=0.6, color='purple')\n",
    "axes[1, 2].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1, 2].set_xlabel('Predicted Values')\n",
    "axes[1, 2].set_ylabel('Residuals')\n",
    "axes[1, 2].set_title('Residual Plot')\n",
    "\n",
    "# 7. Cross-validation scores\n",
    "cv_scores = cross_val_score(xgb_clf, X_wine, y_wine, cv=5, scoring='accuracy')\n",
    "axes[2, 0].bar(range(1, 6), cv_scores, color='orange', alpha=0.7)\n",
    "axes[2, 0].axhline(cv_scores.mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean: {cv_scores.mean():.3f}')\n",
    "axes[2, 0].set_xlabel('Fold')\n",
    "axes[2, 0].set_ylabel('Accuracy')\n",
    "axes[2, 0].set_title('Cross-Validation Scores')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# 8. Tree visualization (first tree)\n",
    "# Note: This shows the structure of the first tree in the ensemble\n",
    "try:\n",
    "    xgb.plot_tree(xgb_native, num_trees=0, ax=axes[2, 1])\n",
    "    axes[2, 1].set_title('First Tree Structure')\n",
    "except:\n",
    "    axes[2, 1].text(0.5, 0.5, 'Tree visualization\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[2, 1].transAxes)\n",
    "    axes[2, 1].set_title('Tree Structure (Not Available)')\n",
    "\n",
    "# 9. Feature Importance Distribution\n",
    "axes[2, 2].hist(xgb_clf.feature_importances_, bins=10, alpha=0.7, color='green')\n",
    "axes[2, 2].set_xlabel('Feature Importance')\n",
    "axes[2, 2].set_ylabel('Number of Features')\n",
    "axes[2, 2].set_title('Distribution of Feature Importances')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Cross-Validation Results:\")\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa57998",
   "metadata": {},
   "source": [
    "## **üéõÔ∏è 8. Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéõÔ∏è XGBoost Hyperparameter Tuning\")\n",
    "\n",
    "# Define parameter space for tuning\n",
    "print(\"Key XGBoost Hyperparameters:\")\n",
    "print(\"\"\"\n",
    "1. n_estimators: Number of boosting rounds\n",
    "2. max_depth: Maximum tree depth\n",
    "3. learning_rate (eta): Step size shrinkage\n",
    "4. subsample: Fraction of samples for each tree\n",
    "5. colsample_bytree: Fraction of features for each tree\n",
    "6. gamma: Minimum loss reduction for split\n",
    "7. reg_alpha: L1 regularization\n",
    "8. reg_lambda: L2 regularization\n",
    "\"\"\")\n",
    "\n",
    "# Grid Search with key parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Use a sample for faster computation\n",
    "X_sample, _, y_sample, _ = train_test_split(X_wine, y_wine, train_size=0.4, random_state=42)\n",
    "\n",
    "print(\"üîç Grid Search (limited parameters for demo)\")\n",
    "grid_search = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Random Search for more comprehensive tuning\n",
    "print(\"\\nüé≤ Random Search\")\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "random_param_dist = {\n",
    "    'n_estimators': randint(50, 301),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(1, 2)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "    random_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.3f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_xgb = XGBClassifier(**random_search.best_params_, random_state=42)\n",
    "best_xgb.fit(X_train_wine, y_train_wine)\n",
    "best_predictions = best_xgb.predict(X_test_wine)\n",
    "\n",
    "print(f\"\\nüèÜ Tuned Model Performance:\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test_wine, best_predictions):.3f}\")\n",
    "print(f\"Original XGBoost: {accuracy_score(y_test_wine, y_test_pred):.3f}\")\n",
    "print(f\"Improvement: {accuracy_score(y_test_wine, best_predictions) - accuracy_score(y_test_wine, y_test_pred):+.3f}\")\n",
    "\n",
    "# Analyze parameter importance\n",
    "print(\"\\nüìä Parameter Analysis:\")\n",
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "param_cols = [col for col in results_df.columns if col.startswith('param_')]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot parameter distributions vs score\n",
    "for i, param in enumerate(['param_n_estimators', 'param_max_depth', 'param_learning_rate']):\n",
    "    if param in results_df.columns:\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.scatter(results_df[param], results_df['mean_test_score'], alpha=0.6)\n",
    "        plt.xlabel(param.replace('param_', ''))\n",
    "        plt.ylabel('CV Score')\n",
    "        plt.title(f'CV Score vs {param.replace(\"param_\", \"\")}')\n",
    "\n",
    "for i, param in enumerate(['param_subsample', 'param_colsample_bytree', 'param_gamma']):\n",
    "    if param in results_df.columns:\n",
    "        plt.subplot(2, 3, i+4)\n",
    "        plt.scatter(results_df[param], results_df['mean_test_score'], alpha=0.6)\n",
    "        plt.xlabel(param.replace('param_', ''))\n",
    "        plt.ylabel('CV Score')\n",
    "        plt.title(f'CV Score vs {param.replace(\"param_\", \"\")}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1c15e",
   "metadata": {},
   "source": [
    "## **‚öñÔ∏è 9. Bias-Variance and Regularization Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544eb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è Bias-Variance and Regularization Analysis\")\n",
    "\n",
    "# Analyze effect of learning rate\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "lr_scores = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    xgb_lr = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    score = cross_val_score(xgb_lr, X_wine, y_wine, cv=3).mean()\n",
    "    lr_scores.append(score)\n",
    "\n",
    "# Analyze effect of regularization\n",
    "reg_lambdas = [0, 0.1, 0.5, 1, 2, 5, 10]\n",
    "reg_scores = []\n",
    "\n",
    "for reg_lambda in reg_lambdas:\n",
    "    xgb_reg = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        reg_lambda=reg_lambda,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    score = cross_val_score(xgb_reg, X_wine, y_wine, cv=3).mean()\n",
    "    reg_scores.append(score)\n",
    "\n",
    "# Analyze effect of tree depth\n",
    "depths = range(1, 11)\n",
    "depth_train_scores = []\n",
    "depth_test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    xgb_depth = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=depth,\n",
    "        random_state=42,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    xgb_depth.fit(X_train_wine, y_train_wine)\n",
    "    \n",
    "    train_score = xgb_depth.score(X_train_wine, y_train_wine)\n",
    "    test_score = xgb_depth.score(X_test_wine, y_test_wine)\n",
    "    \n",
    "    depth_train_scores.append(train_score)\n",
    "    depth_test_scores.append(test_score)\n",
    "\n",
    "# Plot analysis results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(learning_rates, lr_scores, 'o-', color='blue', linewidth=2)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('CV Score')\n",
    "plt.title('Effect of Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(reg_lambdas, reg_scores, 'o-', color='red', linewidth=2)\n",
    "plt.xlabel('L2 Regularization (lambda)')\n",
    "plt.ylabel('CV Score')\n",
    "plt.title('Effect of L2 Regularization')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(depths, depth_train_scores, 'o-', label='Training', color='blue')\n",
    "plt.plot(depths, depth_test_scores, 'o-', label='Testing', color='red')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Overfitting Analysis: Depth')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Early stopping analysis\n",
    "print(\"\\nüõë Early Stopping Analysis\")\n",
    "\n",
    "xgb_early = XGBClassifier(\n",
    "    n_estimators=500,  # Large number\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "eval_set = [(X_train_wine, y_train_wine), (X_test_wine, y_test_wine)]\n",
    "xgb_early.fit(\n",
    "    X_train_wine, y_train_wine,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Optimal number of estimators: {xgb_early.best_iteration}\")\n",
    "print(f\"Best validation score: {xgb_early.best_score:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "results = xgb_early.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), results['validation_0']['mlogloss'], label='Train', linewidth=2)\n",
    "plt.plot(range(epochs), results['validation_1']['mlogloss'], label='Validation', linewidth=2)\n",
    "plt.axvline(x=xgb_early.best_iteration, color='red', linestyle='--', \n",
    "           label=f'Best Iteration ({xgb_early.best_iteration})')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Training History with Early Stopping')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal hyperparameters found:\")\n",
    "print(f\"‚Ä¢ Learning Rate: {0.1} (moderate for stability)\")\n",
    "print(f\"‚Ä¢ Optimal Lambda: {reg_lambdas[np.argmax(reg_scores)]}\")\n",
    "print(f\"‚Ä¢ Optimal Depth: {depths[np.argmax(depth_test_scores)]}\")\n",
    "print(f\"‚Ä¢ Early Stopping: {xgb_early.best_iteration} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14883525",
   "metadata": {},
   "source": [
    "## **üÜö 10. Comprehensive Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üÜö Comprehensive Model Comparison\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),\n",
    "    'XGBoost (Tuned)': best_xgb\n",
    "}\n",
    "\n",
    "# Compare across multiple metrics\n",
    "results = []\n",
    "training_times = []\n",
    "prediction_times = []\n",
    "\n",
    "import time\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_wine, y_train_wine)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test_wine)\n",
    "    pred_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_wine, y_pred)\n",
    "    precision = precision_score(y_test_wine, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_wine, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_wine, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_score = cross_val_score(model, X_wine, y_wine, cv=5).mean()\n",
    "    cv_std = cross_val_score(model, X_wine, y_wine, cv=5).std()\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'CV Score': cv_score,\n",
    "        'CV Std': cv_std,\n",
    "        'Train Time': train_time,\n",
    "        'Pred Time': pred_time\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Model Comparison Results:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Performance metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'purple']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    bars = ax.bar(range(len(comparison_df)), comparison_df[metric], color=colors, alpha=0.7)\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xticks(range(len(comparison_df)))\n",
    "    ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Cross-validation scores with error bars\n",
    "ax = axes[1, 0]\n",
    "bars = ax.bar(range(len(comparison_df)), comparison_df['CV Score'], \n",
    "              yerr=comparison_df['CV Std'], color=colors, alpha=0.7, capsize=5)\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('CV Score')\n",
    "ax.set_title('Cross-Validation Scores')\n",
    "ax.set_xticks(range(len(comparison_df)))\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "ax = axes[1, 1]\n",
    "bars = ax.bar(range(len(comparison_df)), comparison_df['Train Time'], color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Training Time (seconds)')\n",
    "ax.set_title('Training Time Comparison')\n",
    "ax.set_xticks(range(len(comparison_df)))\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance comparison between tree-based methods\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'XGBoost (Tuned)']\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Get top 8 features\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': wine.feature_names,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(8)\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], \n",
    "             color=colors[i+1], alpha=0.7)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'{model_name} - Top Features')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance test\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\nüìà Performance Summary:\")\n",
    "best_model = comparison_df.loc[comparison_df['CV Score'].idxmax(), 'Model']\n",
    "best_score = comparison_df['CV Score'].max()\n",
    "\n",
    "print(f\"üèÜ Best Model: {best_model}\")\n",
    "print(f\"üéØ Best CV Score: {best_score:.3f}\")\n",
    "print(f\"üìä Performance Ranking:\")\n",
    "\n",
    "ranking = comparison_df.sort_values('CV Score', ascending=False)\n",
    "for i, (_, row) in enumerate(ranking.iterrows()):\n",
    "    print(f\"{i+1}. {row['Model']}: {row['CV Score']:.3f} ¬± {row['CV Std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3c388",
   "metadata": {},
   "source": [
    "## **üîç 11. Real-World Example: Financial Credit Scoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9604a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic credit scoring dataset\n",
    "print(\"üí≥ Financial Credit Scoring Example\")\n",
    "\n",
    "np.random.seed(42)\n",
    "n_customers = 5000\n",
    "\n",
    "# Generate customer features\n",
    "credit_data = {\n",
    "    'age': np.random.normal(40, 15, n_customers).clip(18, 80),\n",
    "    'income': np.random.lognormal(10, 0.5, n_customers),\n",
    "    'credit_history_length': np.random.exponential(8, n_customers).clip(0, 30),\n",
    "    'existing_loans': np.random.poisson(2, n_customers),\n",
    "    'employment_duration': np.random.exponential(5, n_customers).clip(0, 20),\n",
    "    'debt_to_income': np.random.beta(2, 5, n_customers),\n",
    "    'previous_defaults': np.random.poisson(0.3, n_customers),\n",
    "    'credit_utilization': np.random.beta(2, 3, n_customers),\n",
    "    'savings_balance': np.random.lognormal(8, 1, n_customers),\n",
    "    'property_value': np.random.lognormal(11, 0.8, n_customers)\n",
    "}\n",
    "\n",
    "# Create realistic default probability\n",
    "default_logit = (\n",
    "    -3.0 +  # base (low default rate)\n",
    "    -0.02 * credit_data['age'] +\n",
    "    -0.00001 * credit_data['income'] +\n",
    "    -0.1 * credit_data['credit_history_length'] +\n",
    "    0.3 * credit_data['existing_loans'] +\n",
    "    -0.05 * credit_data['employment_duration'] +\n",
    "    2.0 * credit_data['debt_to_income'] +\n",
    "    0.5 * credit_data['previous_defaults'] +\n",
    "    1.5 * credit_data['credit_utilization'] +\n",
    "    -0.00001 * credit_data['savings_balance'] +\n",
    "    -0.00001 * credit_data['property_value']\n",
    ")\n",
    "\n",
    "# Convert to probability and generate binary outcome\n",
    "default_prob = 1 / (1 + np.exp(-default_logit))\n",
    "default_outcome = (np.random.random(n_customers) < default_prob).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "credit_df = pd.DataFrame(credit_data)\n",
    "credit_df['default'] = default_outcome\n",
    "\n",
    "print(f\"Credit dataset shape: {credit_df.shape}\")\n",
    "print(f\"Default rate: {default_outcome.mean():.1%}\")\n",
    "print(f\"Features: {list(credit_data.keys())}\")\n",
    "\n",
    "# Prepare data\n",
    "feature_columns = list(credit_data.keys())\n",
    "X_credit = credit_df[feature_columns]\n",
    "y_credit = credit_df['default']\n",
    "\n",
    "# Split data\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = train_test_split(\n",
    "    X_credit, y_credit, test_size=0.2, random_state=42, stratify=y_credit\n",
    ")\n",
    "\n",
    "# Train XGBoost for credit scoring\n",
    "credit_xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=len(y_credit[y_credit==0])/len(y_credit[y_credit==1]),  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "eval_set = [(X_train_credit, y_train_credit), (X_test_credit, y_test_credit)]\n",
    "credit_xgb.fit(\n",
    "    X_train_credit, y_train_credit,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_credit = credit_xgb.predict(X_test_credit)\n",
    "y_proba_credit = credit_xgb.predict_proba(X_test_credit)[:, 1]\n",
    "\n",
    "print(\"\\nüéØ Credit Scoring Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_credit, y_pred_credit):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test_credit, y_pred_credit):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test_credit, y_pred_credit):.3f}\")\n",
    "\n",
    "# AUC Score (important for credit scoring)\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "auc_score = roc_auc_score(y_test_credit, y_proba_credit)\n",
    "print(f\"AUC Score: {auc_score:.3f}\")\n",
    "\n",
    "# Feature importance for credit scoring\n",
    "credit_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': credit_xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Most Important Risk Factors:\")\n",
    "for i, (_, row) in enumerate(credit_importance.iterrows()):\n",
    "    print(f\"{i+1}. {row['Feature']}: {row['Importance']:.3f}\")\n",
    "\n",
    "# Visualize credit scoring results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Feature importance\n",
    "axes[0, 0].barh(credit_importance['Feature'], credit_importance['Importance'])\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "axes[0, 0].set_title('Credit Risk Factors')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_credit, y_proba_credit)\n",
    "axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc_score:.3f}')\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve - Credit Scoring')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm_credit = confusion_matrix(y_test_credit, y_pred_credit)\n",
    "sns.heatmap(cm_credit, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['No Default', 'Default'], yticklabels=['No Default', 'Default'])\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "axes[1, 0].set_title('Confusion Matrix')\n",
    "\n",
    "# 4. Score distribution\n",
    "axes[1, 1].hist(y_proba_credit[y_test_credit == 0], bins=50, alpha=0.5, \n",
    "                label='No Default', color='green', density=True)\n",
    "axes[1, 1].hist(y_proba_credit[y_test_credit == 1], bins=50, alpha=0.5, \n",
    "                label='Default', color='red', density=True)\n",
    "axes[1, 1].set_xlabel('Default Probability')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Score Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Business interpretation\n",
    "print(\"\\nüíº Business Interpretation:\")\n",
    "print(\"Key risk indicators for credit defaults:\")\n",
    "top_3_features = credit_importance.head(3)\n",
    "for i, (_, row) in enumerate(top_3_features.iterrows()):\n",
    "    print(f\"{i+1}. {row['Feature']}: Primary risk factor\")\n",
    "\n",
    "print(f\"\\nModel Performance for Business:\")\n",
    "print(f\"‚Ä¢ AUC of {auc_score:.3f} indicates {'excellent' if auc_score > 0.8 else 'good'} discriminatory power\")\n",
    "print(f\"‚Ä¢ Can identify {recall_score(y_test_credit, y_pred_credit):.1%} of actual defaults\")\n",
    "print(f\"‚Ä¢ {precision_score(y_test_credit, y_pred_credit):.1%} of flagged cases are true defaults\")\n",
    "print(f\"‚Ä¢ Suitable for automated credit decisioning with human oversight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27856b06",
   "metadata": {},
   "source": [
    "## **‚úÖ 12. Advantages and Disadvantages**\n",
    "\n",
    "### **XGBoost Advantages:** ‚úÖ\n",
    "\n",
    "1. **High Performance**: Often achieves state-of-the-art results\n",
    "2. **Regularization**: Built-in L1 and L2 regularization prevents overfitting\n",
    "3. **Handles Missing Values**: Native support for missing data\n",
    "4. **Feature Importance**: Multiple importance metrics (gain, cover, weight)\n",
    "5. **Flexibility**: Works for classification, regression, and ranking\n",
    "6. **Parallel Processing**: Optimized for speed and scalability\n",
    "7. **Early Stopping**: Built-in early stopping prevents overfitting\n",
    "8. **Cross-Validation**: Native cross-validation support\n",
    "9. **Memory Efficient**: Optimized memory usage\n",
    "10. **Robustness**: Handles outliers well through tree-based learning\n",
    "\n",
    "### **XGBoost Disadvantages:** ‚ùå\n",
    "\n",
    "1. **Hyperparameter Complexity**: Many parameters to tune\n",
    "2. **Computational Cost**: Can be slower than simpler models\n",
    "3. **Black Box**: Less interpretable than linear models or single trees\n",
    "4. **Overfitting Risk**: Can overfit with small datasets or poor parameters\n",
    "5. **Memory Usage**: Requires more memory than simple models\n",
    "6. **Learning Curve**: Steeper learning curve than Random Forest\n",
    "\n",
    "### **When to Use XGBoost:** üéØ\n",
    "\n",
    "- **Competitive ML**: When you need maximum predictive performance\n",
    "- **Tabular Data**: Excellent for structured/tabular datasets\n",
    "- **Feature Rich**: When you have many features and complex relationships\n",
    "- **Class Imbalance**: Good handling of imbalanced datasets\n",
    "- **Large Datasets**: Scales well with large amounts of data\n",
    "- **Ensemble Models**: As part of ensemble learning strategies\n",
    "\n",
    "### **When NOT to Use XGBoost:** ‚ö†Ô∏è\n",
    "\n",
    "- **Simple Relationships**: When linear models would suffice\n",
    "- **Real-time Inference**: When prediction speed is critical\n",
    "- **Small Datasets**: Risk of overfitting with limited data\n",
    "- **High Interpretability Required**: When you need to explain every decision\n",
    "- **Resource Constraints**: Limited computational resources\n",
    "- **Image/Text Data**: Deep learning often better for unstructured data\n",
    "\n",
    "### **XGBoost vs Other Methods:**\n",
    "\n",
    "| Aspect | Decision Tree | Random Forest | XGBoost |\n",
    "|--------|---------------|---------------|---------|\n",
    "| **Accuracy** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Speed** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Interpretability** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |\n",
    "| **Overfitting Resistance** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Feature Handling** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Hyperparameter Sensitivity** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725c44",
   "metadata": {},
   "source": [
    "## **üìù 13. Summary and Key Takeaways**\n",
    "\n",
    "### **XGBoost in a Nutshell:** üå∞\n",
    "\n",
    "XGBoost is a powerful, optimized gradient boosting framework that sequentially builds trees to correct errors of previous trees, incorporating advanced regularization and optimization techniques for superior performance.\n",
    "\n",
    "### **Key Concepts Mastered:** üí°\n",
    "\n",
    "1. **Gradient Boosting**: Sequential learning that corrects previous errors\n",
    "2. **Regularization**: L1/L2 penalties prevent overfitting\n",
    "3. **Tree Pruning**: Intelligent pruning improves generalization\n",
    "4. **Learning Rate**: Controls step size for stable convergence\n",
    "5. **Subsampling**: Reduces overfitting and improves speed\n",
    "6. **Early Stopping**: Prevents overtraining automatically\n",
    "7. **Feature Importance**: Multiple metrics for feature analysis\n",
    "\n",
    "### **Essential Hyperparameters:** üéõÔ∏è\n",
    "\n",
    "**Core Parameters:**\n",
    "- `n_estimators`: Number of boosting rounds (50-500)\n",
    "- `max_depth`: Tree depth (3-10)\n",
    "- `learning_rate`: Step size (0.01-0.3)\n",
    "\n",
    "**Regularization:**\n",
    "- `reg_lambda`: L2 regularization (1-10)\n",
    "- `reg_alpha`: L1 regularization (0-1)\n",
    "- `gamma`: Minimum loss reduction (0-0.5)\n",
    "\n",
    "**Sampling:**\n",
    "- `subsample`: Row sampling (0.6-1.0)\n",
    "- `colsample_bytree`: Column sampling (0.6-1.0)\n",
    "\n",
    "### **Best Practices:** üéØ\n",
    "\n",
    "1. **Start Simple**: Begin with default parameters\n",
    "2. **Use Cross-Validation**: Always validate with CV\n",
    "3. **Early Stopping**: Prevent overfitting with early stopping\n",
    "4. **Handle Imbalance**: Use `scale_pos_weight` for imbalanced data\n",
    "5. **Monitor Learning**: Watch training curves for overfitting\n",
    "6. **Feature Engineering**: Good features still matter\n",
    "7. **Hyperparameter Tuning**: Use systematic tuning approaches\n",
    "\n",
    "### **Performance Optimization:** ‚ö°\n",
    "\n",
    "- **Parallel Processing**: Set `n_jobs=-1`\n",
    "- **Memory Management**: Use `tree_method='hist'` for large datasets\n",
    "- **Early Stopping**: Save time with `early_stopping_rounds`\n",
    "- **Approximate Methods**: Use `tree_method='approx'` for speed\n",
    "\n",
    "### **Comparison Summary:**\n",
    "\n",
    "| Model | Accuracy | Speed | Interpretability | Use Case |\n",
    "|-------|----------|-------|------------------|----------|\n",
    "| **Decision Tree** | Good | Fast | High | Simple, interpretable models |\n",
    "| **Random Forest** | Better | Medium | Medium | Robust, general-purpose |\n",
    "| **XGBoost** | Best | Medium | Medium | Maximum performance needed |\n",
    "\n",
    "### **Real-World Applications:** üåç\n",
    "\n",
    "- **Finance**: Credit scoring, fraud detection\n",
    "- **Healthcare**: Medical diagnosis, drug discovery\n",
    "- **Marketing**: Customer segmentation, churn prediction\n",
    "- **E-commerce**: Recommendation systems, pricing\n",
    "- **Technology**: Ranking algorithms, CTR prediction\n",
    "\n",
    "### **Next Steps:** üöÄ\n",
    "\n",
    "1. **Advanced XGBoost**: Explore XGBoost advanced features\n",
    "2. **LightGBM/CatBoost**: Try other gradient boosting libraries\n",
    "3. **Ensemble Methods**: Combine XGBoost with other models\n",
    "4. **Feature Engineering**: Master feature creation techniques\n",
    "5. **AutoML**: Explore automated machine learning tools\n",
    "6. **Deep Learning**: Learn neural networks for complex patterns\n",
    "\n",
    "### **Final Thoughts:** üí≠\n",
    "\n",
    "XGBoost represents the pinnacle of tree-based machine learning, offering:\n",
    "- **State-of-the-art performance** on tabular data\n",
    "- **Robust regularization** preventing overfitting\n",
    "- **Efficient implementation** for production use\n",
    "- **Rich ecosystem** with extensive documentation\n",
    "\n",
    "While it requires more expertise than simpler models, XGBoost often provides the best performance for structured data problems, making it a essential tool in any data scientist's toolkit! üöÄ‚ú®\n",
    "\n",
    "**Remember**: The best model is the one that solves your business problem effectively - sometimes that's XGBoost, sometimes it's a simple linear model. Always start with understanding your problem and data first! üéØ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
