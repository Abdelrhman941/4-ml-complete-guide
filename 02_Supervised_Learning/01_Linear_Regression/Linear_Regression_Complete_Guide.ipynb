{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7baf57e",
   "metadata": {},
   "source": [
    "# üìà Linear Regression - Complete Guide\n",
    "\n",
    "This notebook provides a comprehensive introduction to Linear Regression, covering:\n",
    "- Mathematical foundation\n",
    "- Implementation from scratch\n",
    "- Scikit-learn implementation\n",
    "- Model evaluation and interpretation\n",
    "- Assumptions checking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0fd58",
   "metadata": {},
   "source": [
    "## üìö 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94759ed3",
   "metadata": {},
   "source": [
    "## üßÆ 2. Mathematical Foundation\n",
    "\n",
    "### Simple Linear Regression\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "### Multiple Linear Regression\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "### Normal Equation\n",
    "$$\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "### Cost Function (MSE)\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_{\\boldsymbol{\\beta}}(x^{(i)}) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d489ac7",
   "metadata": {},
   "source": [
    "## üìä 3. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 1) * 2\n",
    "y = 3 + 2 * X.ravel() + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "df = pd.DataFrame({\n",
    "    'feature': X.ravel(),\n",
    "    'target': y\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, color='blue')\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Sample Dataset for Linear Regression')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf2451",
   "metadata": {},
   "source": [
    "## üîß 4. Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d56e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation from scratch using Normal Equation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model using Normal Equation\n",
    "        \"\"\"\n",
    "        # Add bias column (intercept term)\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # Normal equation: Œ≤ = (X^T X)^(-1) X^T y\n",
    "        XtX = X_with_bias.T @ X_with_bias\n",
    "        Xty = X_with_bias.T @ y\n",
    "        \n",
    "        # Solve for coefficients\n",
    "        beta = np.linalg.solve(XtX, Xty)\n",
    "        \n",
    "        # Store intercept and coefficients\n",
    "        self.intercept = beta[0]\n",
    "        self.coefficients = beta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the fitted model\n",
    "        \"\"\"\n",
    "        return self.intercept + X @ self.coefficients\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R¬≤ score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test our implementation\n",
    "model_scratch = LinearRegressionScratch()\n",
    "model_scratch.fit(X, y)\n",
    "\n",
    "print(f\"Intercept: {model_scratch.intercept:.4f}\")\n",
    "print(f\"Coefficient: {model_scratch.coefficients[0]:.4f}\")\n",
    "print(f\"R¬≤ Score: {model_scratch.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2355b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Scikit-learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and fit the model\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model_sklearn.predict(X_train)\n",
    "y_test_pred = model_sklearn.predict(X_test)\n",
    "\n",
    "print(\"üìä Model Parameters:\")\n",
    "print(f\"Intercept: {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"Coefficient: {model_sklearn.coef_[0]:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Performance Metrics:\")\n",
    "print(f\"Training R¬≤: {r2_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Test R¬≤: {r2_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"Training RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56735454",
   "metadata": {},
   "source": [
    "## üìä 6. Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b071c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Regression line\n",
    "axes[0, 0].scatter(X_train, y_train, alpha=0.6, label='Training data', color='blue')\n",
    "axes[0, 0].scatter(X_test, y_test, alpha=0.6, label='Test data', color='red')\n",
    "\n",
    "# Plot regression line\n",
    "X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_range_pred = model_sklearn.predict(X_range)\n",
    "axes[0, 0].plot(X_range, y_range_pred, color='green', linewidth=2, label='Regression line')\n",
    "\n",
    "axes[0, 0].set_xlabel('Feature (X)')\n",
    "axes[0, 0].set_ylabel('Target (y)')\n",
    "axes[0, 0].set_title('Linear Regression Fit')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Fitted\n",
    "residuals = y_test - y_test_pred\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6)\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Fitted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals vs Fitted Values')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q plot for residuals\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot of Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Actual vs Predicted\n",
    "axes[1, 1].scatter(y_test, y_test_pred, alpha=0.6)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1, 1].set_xlabel('Actual Values')\n",
    "axes[1, 1].set_ylabel('Predicted Values')\n",
    "axes[1, 1].set_title('Actual vs Predicted Values')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41118981",
   "metadata": {},
   "source": [
    "## ‚úÖ 7. Assumption Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17280c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_assumptions(X, y, model, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Check linear regression assumptions\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    print(\"üîç LINEAR REGRESSION ASSUMPTIONS CHECK\\n\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Linearity (correlation between X and y)\n",
    "    correlation = np.corrcoef(X.ravel(), y)[0, 1]\n",
    "    print(f\"1. LINEARITY\")\n",
    "    print(f\"   Correlation coefficient: {correlation:.4f}\")\n",
    "    print(f\"   Status: {'‚úÖ GOOD' if abs(correlation) > 0.3 else '‚ö†Ô∏è WEAK LINEAR RELATIONSHIP'}\\n\")\n",
    "    \n",
    "    # 2. Independence (Durbin-Watson test)\n",
    "    from statsmodels.stats.diagnostic import durbin_watson\n",
    "    dw_stat = durbin_watson(residuals)\n",
    "    print(f\"2. INDEPENDENCE\")\n",
    "    print(f\"   Durbin-Watson statistic: {dw_stat:.4f}\")\n",
    "    print(f\"   Status: {'‚úÖ GOOD' if 1.5 < dw_stat < 2.5 else '‚ö†Ô∏è POTENTIAL AUTOCORRELATION'}\\n\")\n",
    "    \n",
    "    # 3. Homoscedasticity (Breusch-Pagan test)\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    _, bp_pvalue, _, _ = het_breuschpagan(residuals, X)\n",
    "    print(f\"3. HOMOSCEDASTICITY\")\n",
    "    print(f\"   Breusch-Pagan test p-value: {bp_pvalue:.4f}\")\n",
    "    print(f\"   Status: {'‚úÖ GOOD' if bp_pvalue > alpha else '‚ö†Ô∏è HETEROSCEDASTICITY DETECTED'}\\n\")\n",
    "    \n",
    "    # 4. Normality of residuals (Shapiro-Wilk test)\n",
    "    _, shapiro_pvalue = stats.shapiro(residuals)\n",
    "    print(f\"4. NORMALITY OF RESIDUALS\")\n",
    "    print(f\"   Shapiro-Wilk test p-value: {shapiro_pvalue:.4f}\")\n",
    "    print(f\"   Status: {'‚úÖ GOOD' if shapiro_pvalue > alpha else '‚ö†Ô∏è RESIDUALS NOT NORMALLY DISTRIBUTED'}\\n\")\n",
    "    \n",
    "    # 5. No multicollinearity (for multiple features)\n",
    "    if X.shape[1] > 1:\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = [f\"X{i}\" for i in range(X.shape[1])]\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "        \n",
    "        print(f\"5. MULTICOLLINEARITY\")\n",
    "        print(vif_data)\n",
    "        max_vif = vif_data[\"VIF\"].max()\n",
    "        print(f\"   Status: {'‚úÖ GOOD' if max_vif < 5 else '‚ö†Ô∏è HIGH MULTICOLLINEARITY'}\\n\")\n",
    "    else:\n",
    "        print(f\"5. MULTICOLLINEARITY\")\n",
    "        print(f\"   Single feature - not applicable\\n\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"‚úÖ = Assumption satisfied\")\n",
    "    print(\"‚ö†Ô∏è = Assumption violated or needs attention\")\n",
    "\n",
    "# Check assumptions\n",
    "check_assumptions(X_test, y_test, model_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e3933",
   "metadata": {},
   "source": [
    "## üéØ 8. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model_sklearn, X, y, cv=5, scoring='r2')\n",
    "cv_rmse = -cross_val_score(model_sklearn, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print(\"üîÑ Cross-Validation Results:\")\n",
    "print(f\"R¬≤ Scores: {cv_scores}\")\n",
    "print(f\"Mean R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nRMSE Scores: {cv_rmse}\")\n",
    "print(f\"Mean RMSE: {cv_rmse.mean():.4f} (+/- {cv_rmse.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize cross-validation scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].boxplot(cv_scores)\n",
    "axes[0].set_title('Cross-Validation R¬≤ Scores')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot(cv_rmse)\n",
    "axes[1].set_title('Cross-Validation RMSE Scores')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddc871",
   "metadata": {},
   "source": [
    "## üìù 9. Key Takeaways and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã LINEAR REGRESSION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä MODEL PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {r2_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ MODEL PARAMETERS:\")\n",
    "print(f\"   ‚Ä¢ Intercept (Œ≤‚ÇÄ): {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"   ‚Ä¢ Slope (Œ≤‚ÇÅ): {model_sklearn.coef_[0]:.4f}\")\n",
    "print(f\"   ‚Ä¢ Equation: y = {model_sklearn.intercept_:.4f} + {model_sklearn.coef_[0]:.4f}x\")\n",
    "\n",
    "print(f\"\\nüìà INTERPRETATION:\")\n",
    "print(f\"   ‚Ä¢ For every 1 unit increase in X, y increases by {model_sklearn.coef_[0]:.4f} units\")\n",
    "print(f\"   ‚Ä¢ When X = 0, the predicted value of y is {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"   ‚Ä¢ The model explains {r2_score(y_test, y_test_pred)*100:.1f}% of the variance in y\")\n",
    "\n",
    "print(f\"\\n‚úÖ NEXT STEPS:\")\n",
    "print(f\"   ‚Ä¢ Verify all assumptions are met\")\n",
    "print(f\"   ‚Ä¢ Consider polynomial features if non-linearity exists\")\n",
    "print(f\"   ‚Ä¢ Try regularized regression (Ridge/Lasso) for multiple features\")\n",
    "print(f\"   ‚Ä¢ Collect more data if performance is insufficient\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efedbe",
   "metadata": {},
   "source": [
    "## üöÄ 10. Exercises\n",
    "\n",
    "### Exercise 1: Multiple Linear Regression\n",
    "- Generate a dataset with 3 features\n",
    "- Fit a multiple linear regression model\n",
    "- Analyze feature importance\n",
    "\n",
    "### Exercise 2: Polynomial Regression\n",
    "- Create non-linear data\n",
    "- Compare linear vs polynomial regression\n",
    "- Find optimal polynomial degree\n",
    "\n",
    "### Exercise 3: Regularized Regression\n",
    "- Implement Ridge and Lasso regression\n",
    "- Compare with regular linear regression\n",
    "- Tune regularization parameter\n",
    "\n",
    "### Exercise 4: Real Dataset\n",
    "- Load Boston Housing or California Housing dataset\n",
    "- Perform complete linear regression analysis\n",
    "- Report insights and recommendations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
