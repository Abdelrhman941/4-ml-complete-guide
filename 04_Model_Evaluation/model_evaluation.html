<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Evaluation - Ultimate Reference</title>
    <style>
        :root {
            --primary-color: #2c7be5;
            --secondary-color: #1a56a0;
            --accent-color: #6ea8fe;
            --text-color: #333;
            --bg-color: #f9f9f9;
            --card-bg: #fff;
            --border-color: #ddd;
            --code-bg: #f5f5f5;
            --table-header: #e9eef6;
            --shadow: 0 4px 6px rgba(0,0,0,0.1);
            --transition: all 0.3s ease;
        }

        .dark-theme {
            --primary-color: #4d8be8;
            --secondary-color: #3a75d0;
            --accent-color: #6ea8fe;
            --text-color: #e0e0e0;
            --bg-color: #121212;
            --card-bg: #1e1e1e;
            --border-color: #333;
            --code-bg: #2d2d2d;
            --table-header: #2c3e50;
            --shadow: 0 4px 6px rgba(0,0,0,0.3);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            transition: var(--transition);
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        .sidebar {
            width: 280px;
            background-color: var(--card-bg);
            border-right: 1px solid var(--border-color);
            padding: 20px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            transition: var(--transition);
            z-index: 100;
        }

        .content {
            flex: 1;
            margin-left: 280px;
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto 0 280px;
        }

        header {
            background-color: var(--primary-color);
            color: white;
            padding: 30px 20px;
            text-align: center;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        h1, h2, h3, h4 {
            margin-bottom: 15px;
            color: var(--primary-color);
        }

        h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
        }

        h2 {
            font-size: 1.8rem;
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 10px;
            margin-top: 40px;
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 25px;
        }

        p {
            margin-bottom: 20px;
            font-size: 1.05rem;
        }

        .card {
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: var(--shadow);
            transition: var(--transition);
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0,0,0,0.12);
        }

        pre {
            background-color: var(--code-bg);
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Consolas', 'Courier New', monospace;
        }

        code {
            font-family: 'Consolas', 'Courier New', monospace;
            background-color: var(--code-bg);
            padding: 2px 5px;
            border-radius: 3px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: var(--shadow);
            border-radius: 8px;
            overflow: hidden;
        }

        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background-color: var(--table-header);
            font-weight: bold;
        }

        tr:nth-child(even) {
            background-color: rgba(0,0,0,0.03);
        }

        tr:hover {
            background-color: rgba(0,0,0,0.05);
        }

        .btn {
            display: inline-block;
            background-color: var(--primary-color);
            color: white;
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            text-decoration: none;
            font-size: 1rem;
            transition: var(--transition);
        }

        .btn:hover {
            background-color: var(--secondary-color);
            transform: translateY(-2px);
        }

        .theme-toggle {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
            background-color: var(--primary-color);
            color: white;
            border: none;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            box-shadow: var(--shadow);
            transition: var(--transition);
        }

        .theme-toggle:hover {
            background-color: var(--secondary-color);
            transform: rotate(30deg);
        }

        .nav-links {
            list-style: none;
        }

        .nav-links li {
            margin-bottom: 10px;
        }

        .nav-links a {
            color: var(--text-color);
            text-decoration: none;
            display: block;
            padding: 8px 10px;
            border-radius: 4px;
            transition: var(--transition);
        }

        .nav-links a:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .collapsible {
            background-color: var(--card-bg);
            color: var(--text-color);
            cursor: pointer;
            padding: 18px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 1.1rem;
            border-radius: 8px;
            margin-bottom: 10px;
            box-shadow: var(--shadow);
            transition: var(--transition);
            position: relative;
        }

        .active, .collapsible:hover {
            background-color: var(--primary-color);
            color: white;
        }

        .collapsible:after {
            content: '\002B';
            color: var(--text-color);
            font-weight: bold;
            float: right;
            margin-left: 5px;
        }

        .active:after {
            content: "\2212";
            color: white;
        }

        .collapsible-content {
            padding: 0 18px;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
            background-color: var(--card-bg);
            border-radius: 0 0 8px 8px;
            margin-bottom: 20px;
        }

        .note {
            background-color: rgba(44, 123, 229, 0.1);
            border-left: 4px solid var(--accent-color);
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .warning {
            background-color: rgba(255, 152, 0, 0.1);
            border-left: 4px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .tip {
            background-color: rgba(76, 175, 80, 0.1);
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }

        .comparison-table {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-card {
            flex: 1;
            min-width: 300px;
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 20px;
            box-shadow: var(--shadow);
        }

        .comparison-card h3 {
            color: var(--primary-color);
            margin-bottom: 15px;
            text-align: center;
        }

        .comparison-card ul {
            padding-left: 20px;
        }

        .comparison-card li {
            margin-bottom: 10px;
        }

        .math {
            font-style: italic;
            font-family: 'Times New Roman', Times, serif;
        }

        .image-container {
            text-align: center;
            margin: 20px 0;
        }

        .image-container img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        .caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9rem;
        }

        .metric-card {
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: var(--shadow);
            border-left: 4px solid var(--primary-color);
        }

        .metric-card h3 {
            color: var(--primary-color);
            margin-bottom: 10px;
        }

        .metric-formula {
            background-color: rgba(44, 123, 229, 0.05);
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
            font-family: 'Times New Roman', Times, serif;
            font-style: italic;
        }

        .confusion-matrix {
            display: grid;
            grid-template-columns: auto auto auto;
            gap: 2px;
            width: fit-content;
            margin: 20px auto;
            font-weight: bold;
        }

        .cm-header {
            background-color: var(--table-header);
            padding: 10px;
            text-align: center;
        }

        .cm-cell {
            background-color: var(--card-bg);
            padding: 15px;
            text-align: center;
            min-width: 100px;
        }

        .cm-tp {
            background-color: rgba(76, 175, 80, 0.2);
        }

        .cm-fp {
            background-color: rgba(255, 152, 0, 0.2);
        }

        .cm-fn {
            background-color: rgba(255, 152, 0, 0.2);
        }

        .cm-tn {
            background-color: rgba(76, 175, 80, 0.2);
        }

        .tabs {
            display: flex;
            margin-bottom: 20px;
        }

        .tab {
            padding: 10px 20px;
            background-color: var(--card-bg);
            border: 1px solid var(--border-color);
            cursor: pointer;
            transition: var(--transition);
        }

        .tab:first-child {
            border-radius: 8px 0 0 8px;
        }

        .tab:last-child {
            border-radius: 0 8px 8px 0;
        }

        .tab.active {
            background-color: var(--primary-color);
            color: white;
            border-color: var(--primary-color);
        }

        .tab-content {
            display: none;
            padding: 20px;
            background-color: var(--card-bg);
            border-radius: 8px;
            box-shadow: var(--shadow);
        }

        .tab-content.active {
            display: block;
        }

        @media (max-width: 768px) {
            .sidebar {
                width: 0;
                padding: 0;
                overflow: hidden;
            }
            
            .content {
                margin-left: 0;
            }
            
            .sidebar.active {
                width: 250px;
                padding: 20px;
            }
            
            .menu-toggle {
                display: block;
                position: fixed;
                top: 20px;
                left: 20px;
                z-index: 1000;
                background-color: var(--primary-color);
                color: white;
                border: none;
                border-radius: 4px;
                padding: 10px;
                cursor: pointer;
            }
        }
    </style>
</head>
<body>
    <button class="theme-toggle" id="themeToggle">ðŸŒ“</button>
    
    <div class="container">
        <aside class="sidebar">
            <h2>Contents</h2>
            <ul class="nav-links">
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#conceptual-explanation">Conceptual Explanation</a></li>
                <li><a href="#classification-metrics">Classification Metrics</a></li>
                <li><a href="#regression-metrics">Regression Metrics</a></li>
                <li><a href="#confusion-matrix">Confusion Matrix</a></li>
                <li><a href="#roc-curves">ROC & PR Curves</a></li>
                <li><a href="#cross-validation">Cross-Validation</a></li>
                <li><a href="#model-selection">Model Selection</a></li>
                <li><a href="#practical-examples">Practical Examples</a></li>
                <li><a href="#common-pitfalls">Common Pitfalls</a></li>
                <li><a href="#best-practices">Best Practices</a></li>
                <li><a href="#resources">Additional Resources</a></li>
            </ul>
        </aside>
        
        <main class="content">
            
            <section id="introduction" class="card">
                <h2>Introduction to Model Evaluation</h2>
                <p>Model evaluation is a critical step in the machine learning pipeline that helps us understand how well our models perform on unseen data. It provides insights into a model's strengths, weaknesses, and areas for improvement.</p>
                
                <div class="note">
                    <strong>Key Point:</strong> Proper model evaluation is essential for building reliable and trustworthy machine learning systems. It helps us select the best model, tune hyperparameters, and estimate how well our model will perform in real-world scenarios.
                </div>
                
                <p>This comprehensive guide covers everything you need to know about model evaluation, from basic metrics to advanced techniques, with practical examples and best practices.</p>
            </section>
            
            <section id="conceptual-explanation" class="card">
                <h2>Conceptual Explanation</h2>
                
                <h3>What is Model Evaluation?</h3>
                <p>Model evaluation is the process of assessing how well a machine learning model performs on data it hasn't seen during training. It helps us:</p>
                
                <ul>
                    <li>Estimate the model's generalization ability</li>
                    <li>Compare different models or algorithms</li>
                    <li>Tune hyperparameters</li>
                    <li>Identify overfitting or underfitting</li>
                    <li>Determine if the model is ready for deployment</li>
                </ul>
                
                <h3>The Evaluation Process</h3>
                <p>A typical model evaluation process involves:</p>
                
                <ol>
                    <li><strong>Data Splitting:</strong> Dividing the dataset into training, validation, and test sets</li>
                    <li><strong>Model Training:</strong> Training the model on the training set</li>
                    <li><strong>Performance Measurement:</strong> Evaluating the model using appropriate metrics</li>
                    <li><strong>Model Comparison:</strong> Comparing different models or configurations</li>
                    <li><strong>Hyperparameter Tuning:</strong> Adjusting model parameters to improve performance</li>
                    <li><strong>Final Evaluation:</strong> Assessing the final model on the test set</li>
                </ol>
                
                <h3>Key Concepts in Model Evaluation</h3>
                
                <button class="collapsible">Training, Validation, and Test Sets</button>
                <div class="collapsible-content">
                    <p>Properly splitting your data is crucial for reliable model evaluation:</p>
                    
                    <ul>
                        <li><strong>Training Set (60-80%):</strong> Used to train the model and learn patterns from the data</li>
                        <li><strong>Validation Set (10-20%):</strong> Used for tuning hyperparameters and making modeling decisions</li>
                        <li><strong>Test Set (10-20%):</strong> Used only once at the end to evaluate the final model's performance</li>
                    </ul>
                    
                    <div class="tip">
                        <strong>Best Practice:</strong> The test set should be representative of the data your model will encounter in production. It should never be used for model training or hyperparameter tuning to avoid data leakage.
                    </div>
                </div>
                
                <button class="collapsible">Overfitting and Underfitting</button>
                <div class="collapsible-content">
                    <p>Model evaluation helps identify these common problems:</p>
                    
                    <ul>
                        <li><strong>Overfitting:</strong> The model performs well on training data but poorly on validation/test data. It has learned the noise in the training data rather than the underlying patterns.</li>
                        <li><strong>Underfitting:</strong> The model performs poorly on both training and validation/test data. It hasn't captured the underlying patterns in the data.</li>
                    </ul>
                    
                    <p>Signs of overfitting include:</p>
                    <ul>
                        <li>High training accuracy but low validation/test accuracy</li>
                        <li>Increasing validation error as training continues</li>
                        <li>Complex model with many parameters relative to the amount of training data</li>
                    </ul>
                    
                    <p>Signs of underfitting include:</p>
                    <ul>
                        <li>Low accuracy on both training and validation/test data</li>
                        <li>High bias in predictions</li>
                        <li>Model is too simple to capture the underlying patterns</li>
                    </ul>
                </div>
                
                <button class="collapsible">Bias-Variance Tradeoff</button>
                <div class="collapsible-content">
                    <p>The bias-variance tradeoff is a fundamental concept in model evaluation:</p>
                    
                    <ul>
                        <li><strong>Bias:</strong> Error due to overly simplistic assumptions in the learning algorithm. High bias can cause underfitting.</li>
                        <li><strong>Variance:</strong> Error due to excessive complexity in the learning algorithm. High variance can cause overfitting.</li>
                    </ul>
                    
                    <p>The goal is to find the sweet spot with the right model complexity that minimizes both bias and variance, resulting in good generalization to new data.</p>
                    
                    <div class="image-container">
                        <svg width="600" height="300" xmlns="http://www.w3.org/2000/svg">
                            <!-- Axes -->
                            <line x1="50" y1="250" x2="550" y2="250" stroke="#333" stroke-width="2"/>
                            <line x1="50" y1="50" x2="50" y2="250" stroke="#333" stroke-width="2"/>
                            
                            <!-- X-axis label -->
                            <text x="300" y="280" text-anchor="middle" font-family="Arial" font-size="14">Model Complexity</text>
                            
                            <!-- Y-axis label -->
                            <text x="20" y="150" text-anchor="middle" font-family="Arial" font-size="14" transform="rotate(-90, 20, 150)">Error</text>
                            
                            <!-- Bias curve -->
                            <path d="M 50,200 Q 150,150 300,120 T 550,100" stroke="#2c7be5" stroke-width="3" fill="none"/>
                            <text x="100" y="190" font-family="Arial" font-size="12" fill="#2c7be5">Bias</text>
                            
                            <!-- Variance curve -->
                            <path d="M 50,100 Q 150,120 300,150 T 550,220" stroke="#ff9800" stroke-width="3" fill="none"/>
                            <text x="500" y="210" font-family="Arial" font-size="12" fill="#ff9800">Variance</text>
                            
                            <!-- Total error curve -->
                            <path d="M 50,180 Q 150,150 300,130 T 550,200" stroke="#4caf50" stroke-width="3" fill="none"/>
                            <text x="300" y="100" font-family="Arial" font-size="12" fill="#4caf50">Total Error</text>
                            
                            <!-- Optimal point -->
                            <circle cx="300" cy="130" r="5" fill="#4caf50"/>
                            <text x="310" y="120" font-family="Arial" font-size="12">Optimal Model Complexity</text>
                        </svg>
                        <p class="caption">Bias-Variance Tradeoff</p>
                    </div>
                </div>
            </section>
            
            <section id="classification-metrics" class="card">
                <h2>Classification Metrics</h2>
                
                <p>Classification metrics are used to evaluate models that predict categorical outcomes. The choice of metric depends on the specific problem, class distribution, and business requirements.</p>
                
                <div class="metric-card">
                    <h3>Accuracy</h3>
                    <p>The proportion of correct predictions among the total number of predictions.</p>
                    
                    <div class="metric-formula">
                        Accuracy = (TP + TN) / (TP + TN + FP + FN)
                    </div>
                    
                    <p><strong>When to use:</strong> When classes are balanced and all types of errors are equally important.</p>
                    
                    <p><strong>Limitations:</strong> Can be misleading with imbalanced classes. For example, in a dataset with 95% negative cases, a model that always predicts "negative" would achieve 95% accuracy without providing any value.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Precision</h3>
                    <p>The proportion of correct positive predictions among all positive predictions. It measures how many of the predicted positives are actually positive.</p>
                    
                    <div class="metric-formula">
                        Precision = TP / (TP + FP)
                    </div>
                    
                    <p><strong>When to use:</strong> When the cost of false positives is high. For example, in spam detection, where falsely marking a legitimate email as spam (false positive) is more problematic than missing some spam emails.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Recall (Sensitivity)</h3>
                    <p>The proportion of correct positive predictions among all actual positives. It measures how many of the actual positives were correctly identified.</p>
                    
                    <div class="metric-formula">
                        Recall = TP / (TP + FN)
                    </div>
                    
                    <p><strong>When to use:</strong> When the cost of false negatives is high. For example, in disease diagnosis, where missing a positive case (false negative) could be life-threatening.</p>
                </div>
                
                <div class="metric-card">
                    <h3>F1 Score</h3>
                    <p>The harmonic mean of precision and recall, providing a balance between the two.</p>
                    
                    <div class="metric-formula">
                        F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
                    </div>
                    
                    <p><strong>When to use:</strong> When you need a balance between precision and recall, especially with imbalanced classes.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Specificity</h3>
                    <p>The proportion of correct negative predictions among all actual negatives.</p>
                    
                    <div class="metric-formula">
                        Specificity = TN / (TN + FP)
                    </div>
                    
                    <p><strong>When to use:</strong> When correctly identifying negative cases is important.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Area Under the ROC Curve (AUC-ROC)</h3>
                    <p>Measures the model's ability to distinguish between classes across all possible classification thresholds.</p>
                    
                    <p><strong>When to use:</strong> When you want to evaluate the model's performance across different thresholds, especially with imbalanced classes.</p>
                    
                    <p><strong>Interpretation:</strong></p>
                    <ul>
                        <li>AUC = 0.5: No better than random guessing</li>
                        <li>0.5 < AUC < 0.7: Poor discrimination</li>
                        <li>0.7 â‰¤ AUC < 0.8: Acceptable discrimination</li>
                        <li>0.8 â‰¤ AUC < 0.9: Excellent discrimination</li>
                        <li>AUC â‰¥ 0.9: Outstanding discrimination</li>
                    </ul>
                </div>
                
                <div class="metric-card">
                    <h3>Area Under the Precision-Recall Curve (AUC-PR)</h3>
                    <p>Similar to AUC-ROC but focuses on the tradeoff between precision and recall.</p>
                    
                    <p><strong>When to use:</strong> When dealing with highly imbalanced datasets where AUC-ROC might be overly optimistic.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Log Loss (Cross-Entropy Loss)</h3>
                    <p>Measures the performance of a classification model where the prediction is a probability value between 0 and 1.</p>
                    
                    <div class="metric-formula">
                        Log Loss = -1/N * Î£[y_i * log(p_i) + (1 - y_i) * log(1 - p_i)]
                    </div>
                    
                    <p><strong>When to use:</strong> When you need a more nuanced evaluation of probabilistic predictions rather than just binary outcomes.</p>
                </div>
                
                <div class="tip">
                    <strong>Choosing the Right Metric:</strong> The choice of evaluation metric should be driven by the specific requirements of your problem. Consider the class distribution, the relative importance of different types of errors, and the business context.
                </div>
            </section>
            
            <section id="regression-metrics" class="card">
                <h2>Regression Metrics</h2>
                
                <p>Regression metrics are used to evaluate models that predict continuous numerical values.</p>
                
                <div class="metric-card">
                    <h3>Mean Absolute Error (MAE)</h3>
                    <p>The average of the absolute differences between predicted and actual values.</p>
                    
                    <div class="metric-formula">
                        MAE = (1/n) * Î£|y_i - Å·_i|
                    </div>
                    
                    <p><strong>When to use:</strong> When you want a straightforward, interpretable metric that treats all errors equally regardless of their direction.</p>
                    
                    <p><strong>Advantages:</strong> Easy to interpret, less sensitive to outliers than MSE.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Mean Squared Error (MSE)</h3>
                    <p>The average of the squared differences between predicted and actual values.</p>
                    
                    <div class="metric-formula">
                        MSE = (1/n) * Î£(y_i - Å·_i)Â²
                    </div>
                    
                    <p><strong>When to use:</strong> When larger errors should be penalized more heavily than smaller ones.</p>
                    
                    <p><strong>Limitations:</strong> Not in the same unit as the target variable, sensitive to outliers.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Root Mean Squared Error (RMSE)</h3>
                    <p>The square root of the MSE, bringing the metric back to the original unit of the target variable.</p>
                    
                    <div class="metric-formula">
                        RMSE = âˆš[(1/n) * Î£(y_i - Å·_i)Â²]
                    </div>
                    
                    <p><strong>When to use:</strong> When you want a metric that is both sensitive to large errors and in the same unit as the target variable.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Mean Absolute Percentage Error (MAPE)</h3>
                    <p>The average of the absolute percentage differences between predicted and actual values.</p>
                    
                    <div class="metric-formula">
                        MAPE = (1/n) * Î£|(y_i - Å·_i) / y_i| * 100%
                    </div>
                    
                    <p><strong>When to use:</strong> When you want to express errors as percentages, making it easier to compare across different scales.</p>
                    
                    <p><strong>Limitations:</strong> Undefined or problematic when actual values are zero or close to zero.</p>
                </div>
                
                <div class="metric-card">
                    <h3>R-squared (Coefficient of Determination)</h3>
                    <p>Represents the proportion of variance in the dependent variable that is predictable from the independent variables.</p>
                    
                    <div class="metric-formula">
                        RÂ² = 1 - (Î£(y_i - Å·_i)Â² / Î£(y_i - È³)Â²)
                    </div>
                    
                    <p><strong>When to use:</strong> When you want to know how well your model explains the variance in the target variable compared to a baseline model.</p>
                    
                    <p><strong>Interpretation:</strong></p>
                    <ul>
                        <li>RÂ² = 1: Perfect fit, all variance explained</li>
                        <li>RÂ² = 0: Model performs no better than predicting the mean</li>
                        <li>RÂ² < 0: Model performs worse than predicting the mean</li>
                    </ul>
                    
                    <p><strong>Limitations:</strong> RÂ² always increases when more variables are added to the model, even if they don't improve predictions.</p>
                </div>
                
                <div class="metric-card">
                    <h3>Adjusted R-squared</h3>
                    <p>A modified version of R-squared that adjusts for the number of predictors in the model.</p>
                    
                    <div class="metric-formula">
                        Adjusted RÂ² = 1 - [(1 - RÂ²) * (n - 1) / (n - p - 1)]
                    </div>
                    
                    <p>where n is the number of samples and p is the number of predictors.</p>
                    
                    <p><strong>When to use:</strong> When comparing models with different numbers of predictors.</p>
                </div>
                
                <div class="comparison-table">
                    <div class="comparison-card">
                        <h3>Comparing Regression Metrics</h3>
                        <table>
                            <tr>
                                <th>Metric</th>
                                <th>Sensitivity to Outliers</th>
                                <th>Interpretability</th>
                                <th>Unit</th>
                            </tr>
                            <tr>
                                <td>MAE</td>
                                <td>Less sensitive</td>
                                <td>High</td>
                                <td>Same as target</td>
                            </tr>
                            <tr>
                                <td>MSE</td>
                                <td>Very sensitive</td>
                                <td>Medium</td>
                                <td>Squared target</td>
                            </tr>
                            <tr>
                                <td>RMSE</td>
                                <td>Sensitive</td>
                                <td>High</td>
                                <td>Same as target</td>
                            </tr>
                            <tr>
                                <td>MAPE</td>
                                <td>Medium</td>
                                <td>High</td>
                                <td>Percentage</td>
                            </tr>
                            <tr>
                                <td>RÂ²</td>
                                <td>Medium</td>
                                <td>Medium</td>
                                <td>Unitless (0-1)</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </section>
            
            <section id="confusion-matrix" class="card">
                <h2>Confusion Matrix</h2>
                
                <p>A confusion matrix is a table that visualizes the performance of a classification model by showing the counts of true positives, false positives, true negatives, and false negatives.</p>
                
                <div class="confusion-matrix">
                    <div class="cm-header"></div>
                    <div class="cm-header">Predicted Positive</div>
                    <div class="cm-header">Predicted Negative</div>
                    
                    <div class="cm-header">Actual Positive</div>
                    <div class="cm-cell cm-tp">True Positive (TP)</div>
                    <div class="cm-cell cm-fn">False Negative (FN)</div>
                    
                    <div class="cm-header">Actual Negative</div>
                    <div class="cm-cell cm-fp">False Positive (FP)</div>
                    <div class="cm-cell cm-tn">True Negative (TN)</div>
                </div>
                
                <h3>Understanding the Confusion Matrix</h3>
                <ul>
                    <li><strong>True Positive (TP):</strong> Model correctly predicts the positive class</li>
                    <li><strong>False Positive (FP):</strong> Model incorrectly predicts the positive class (Type I error)</li>
                    <li><strong>True Negative (TN):</strong> Model correctly predicts the negative class</li>
                    <li><strong>False Negative (FN):</strong> Model incorrectly predicts the negative class (Type II error)</li>
                </ul>
                
                <h3>Metrics Derived from the Confusion Matrix</h3>
                <ul>
                    <li><strong>Accuracy:</strong> (TP + TN) / (TP + TN + FP + FN)</li>
                    <li><strong>Precision:</strong> TP / (TP + FP)</li>
                    <li><strong>Recall (Sensitivity):</strong> TP / (TP + FN)</li>
                    <li><strong>Specificity:</strong> TN / (TN + FP)</li>
                    <li><strong>F1 Score:</strong> 2 * (Precision * Recall) / (Precision + Recall)</li>
                </ul>
                
                <h3>Multi-class Confusion Matrix</h3>
                <p>For problems with more than two classes, the confusion matrix expands to an NÃ—N matrix, where N is the number of classes. Each row represents the instances of an actual class, and each column represents the instances of a predicted class.</p>
                
                <div class="note">
                    <strong>Interpreting a Confusion Matrix:</strong> A good model will have high values along the diagonal (correct predictions) and low values elsewhere (incorrect predictions).
                </div>
                
                <h3>Code Example: Creating a Confusion Matrix</h3>
                <pre><code>
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Create confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=data.target_names, 
            yticklabels=data.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Print classification report
print(classification_report(y_test, y_pred, target_names=data.target_names))</code></pre>
            </section>
            
            <section id="roc-curves" class="card">
                <h2>ROC & PR Curves</h2>
                
                <h3>Receiver Operating Characteristic (ROC) Curve</h3>
                <p>The ROC curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at various classification thresholds.</p>
                
                <div class="image-container">
                    <svg width="400" height="400" xmlns="http://www.w3.org/2000/svg">
                        <!-- Axes -->
                        <line x1="50" y1="350" x2="350" y2="350" stroke="#333" stroke-width="2"/>
                        <line x1="50" y1="350" x2="50" y2="50" stroke="#333" stroke-width="2"/>
                        
                        <!-- X-axis ticks -->
                        <line x1="50" y1="350" x2="50" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="50" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.0</text>
                        
                        <line x1="125" y1="350" x2="125" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="125" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.25</text>
                        
                        <line x1="200" y1="350" x2="200" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="200" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.5</text>
                        
                        <line x1="275" y1="350" x2="275" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="275" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.75</text>
                        
                        <line x1="350" y1="350" x2="350" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="350" y="370" text-anchor="middle" font-family="Arial" font-size="12">1.0</text>
                        
                        <!-- Y-axis ticks -->
                        <line x1="45" y1="350" x2="50" y2="350" stroke="#333" stroke-width="2"/>
                        <text x="35" y="355" text-anchor="end" font-family="Arial" font-size="12">0.0</text>
                        
                        <line x1="45" y1="275" x2="50" y2="275" stroke="#333" stroke-width="2"/>
                        <text x="35" y="280" text-anchor="end" font-family="Arial" font-size="12">0.25</text>
                        
                        <line x1="45" y1="200" x2="50" y2="200" stroke="#333" stroke-width="2"/>
                        <text x="35" y="205" text-anchor="end" font-family="Arial" font-size="12">0.5</text>
                        
                        <line x1="45" y1="125" x2="50" y2="125" stroke="#333" stroke-width="2"/>
                        <text x="35" y="130" text-anchor="end" font-family="Arial" font-size="12">0.75</text>
                        
                        <line x1="45" y1="50" x2="50" y2="50" stroke="#333" stroke-width="2"/>
                        <text x="35" y="55" text-anchor="end" font-family="Arial" font-size="12">1.0</text>
                        
                        <!-- Axis labels -->
                        <text x="200" y="390" text-anchor="middle" font-family="Arial" font-size="14">False Positive Rate</text>
                        <text x="15" y="200" text-anchor="middle" font-family="Arial" font-size="14" transform="rotate(-90, 15, 200)">True Positive Rate</text>
                        
                        <!-- Random classifier line -->
                        <line x1="50" y1="350" x2="350" y2="50" stroke="#999" stroke-width="2" stroke-dasharray="5,5"/>
                        <text x="200" y="220" text-anchor="middle" font-family="Arial" font-size="12" fill="#999">Random Classifier</text>
                        
                        <!-- ROC curve -->
                        <path d="M 50,350 Q 70,150 120,100 T 200,70 T 350,50" stroke="#2c7be5" stroke-width="3" fill="none"/>
                        
                        <!-- AUC label -->
                        <text x="150" y="150" font-family="Arial" font-size="14" fill="#2c7be5">AUC = 0.89</text>
                    </svg>
                    <p class="caption">Example ROC Curve</p>
                </div>
                
                <p><strong>Key characteristics:</strong></p>
                <ul>
                    <li>The diagonal line represents a random classifier (AUC = 0.5)</li>
                    <li>The closer the curve is to the top-left corner, the better the model (AUC closer to 1.0)</li>
                    <li>AUC (Area Under the Curve) provides a single metric for model comparison</li>
                </ul>
                
                <h3>Precision-Recall (PR) Curve</h3>
                <p>The PR curve plots Precision against Recall at various classification thresholds.</p>
                
                <div class="image-container">
                    <svg width="400" height="400" xmlns="http://www.w3.org/2000/svg">
                        <!-- Axes -->
                        <line x1="50" y1="350" x2="350" y2="350" stroke="#333" stroke-width="2"/>
                        <line x1="50" y1="350" x2="50" y2="50" stroke="#333" stroke-width="2"/>
                        
                        <!-- X-axis ticks -->
                        <line x1="50" y1="350" x2="50" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="50" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.0</text>
                        
                        <line x1="125" y1="350" x2="125" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="125" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.25</text>
                        
                        <line x1="200" y1="350" x2="200" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="200" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.5</text>
                        
                        <line x1="275" y1="350" x2="275" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="275" y="370" text-anchor="middle" font-family="Arial" font-size="12">0.75</text>
                        
                        <line x1="350" y1="350" x2="350" y2="355" stroke="#333" stroke-width="2"/>
                        <text x="350" y="370" text-anchor="middle" font-family="Arial" font-size="12">1.0</text>
                        
                        <!-- Y-axis ticks -->
                        <line x1="45" y1="350" x2="50" y2="350" stroke="#333" stroke-width="2"/>
                        <text x="35" y="355" text-anchor="end" font-family="Arial" font-size="12">0.0</text>
                        
                        <line x1="45" y1="275" x2="50" y2="275" stroke="#333" stroke-width="2"/>
                        <text x="35" y="280" text-anchor="end" font-family="Arial" font-size="12">0.25</text>
                        
                        <line x1="45" y1="200" x2="50" y2="200" stroke="#333" stroke-width="2"/>
                        <text x="35" y="205" text-anchor="end" font-family="Arial" font-size="12">0.5</text>
                        
                        <line x1="45" y1="125" x2="50" y2="125" stroke="#333" stroke-width="2"/>
                        <text x="35" y="130" text-anchor="end" font-family="Arial" font-size="12">0.75</text>
                        
                        <line x1="45" y1="50" x2="50" y2="50" stroke="#333" stroke-width="2"/>
                        <text x="35" y="55" text-anchor="end" font-family="Arial" font-size="12">1.0</text>
                        
                        <!-- Axis labels -->
                        <text x="200" y="390" text-anchor="middle" font-family="Arial" font-size="14">Recall</text>
                        <text x="15" y="200" text-anchor="middle" font-family="Arial" font-size="14" transform="rotate(-90, 15, 200)">Precision</text>
                        
                        <!-- PR curve -->
                        <path d="M 50,50 L 100,60 L 150,80 L 200,120 L 250,180 L 300,250 L 350,350" stroke="#ff9800" stroke-width="3" fill="none"/>
                        
                        <!-- AUC-PR label -->
                        <text x="150" y="100" font-family="Arial" font-size="14" fill="#ff9800">AUC-PR = 0.78</text>
                    </svg>
                    <p class="caption">Example Precision-Recall Curve</p>
                </div>
                
                <p><strong>Key characteristics:</strong></p>
                <ul>
                    <li>Focuses on the positive class, making it useful for imbalanced datasets</li>
                    <li>Does not consider true negatives</li>
                    <li>The closer the curve is to the top-right corner, the better the model</li>
                </ul>
                
                <div class="comparison-table">
                    <div class="comparison-card">
                        <h3>ROC Curve vs. PR Curve</h3>
                        <h4>ROC Curve:</h4>
                        <ul>
                            <li>Plots TPR vs. FPR</li>
                            <li>Less sensitive to class imbalance</li>
                            <li>Good for balanced datasets or when both classes are equally important</li>
                        </ul>
                        <h4>PR Curve:</h4>
                        <ul>
                            <li>Plots Precision vs. Recall</li>
                            <li>More sensitive to class imbalance</li>
                            <li>Better for imbalanced datasets or when the positive class is more important</li>
                        </ul>
                        <h4>When to use which:</h4>
                        <p>Use ROC curves when working with balanced datasets or when both classes are equally important. Use PR curves when working with imbalanced datasets or when the positive class is more important than the negative class.</p>
                    </div>
                </div>
                
                <h3>Code Example: ROC and PR Curves</h3>
                <pre><code>
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

# Generate a binary classification dataset
X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1],
                        n_features=20, random_state=42)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Get predicted probabilities
y_scores = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC
fpr, tpr, thresholds_roc = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

# Calculate PR curve and AUC
precision, recall, thresholds_pr = precision_recall_curve(y_test, y_scores)
pr_auc = average_precision_score(y_test, y_scores)

# Plot ROC curve
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)

# Plot PR curve
plt.subplot(1, 2, 2)
plt.plot(recall, precision, color='orange', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Plot threshold analysis
plt.figure(figsize=(12, 5))

# ROC threshold analysis
plt.subplot(1, 2, 1)
plt.plot(thresholds_roc, tpr[:-1], 'b-', label='True Positive Rate')
plt.plot(thresholds_roc, fpr[:-1], 'r-', label='False Positive Rate')
plt.xlabel('Threshold')
plt.ylabel('Rate')
plt.title('ROC Threshold Analysis')
plt.legend()
plt.grid(alpha=0.3)

# PR threshold analysis
plt.subplot(1, 2, 2)
plt.plot(thresholds_pr, precision[:-1], 'g-', label='Precision')
plt.plot(thresholds_pr, recall[:-1], 'm-', label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('PR Threshold Analysis')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
            </section>
            
            <section id="cross-validation" class="card">
                <h2>Cross-Validation</h2>
                
                <p>Cross-validation is a resampling technique used to evaluate machine learning models on limited data. It helps assess how well a model will generalize to an independent dataset.</p>
                
                <h3>K-Fold Cross-Validation</h3>
                <p>In K-fold cross-validation, the dataset is divided into K equal parts (folds). The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times, with each fold serving as the validation set once.</p>
                
                <div class="image-container">
                    <svg width="600" height="300" xmlns="http://www.w3.org/2000/svg">
                        <!-- Fold 1 -->
                        <rect x="50" y="250" width="400" height="40" fill="#2c7be5" stroke="#333" />
                        <rect x="450" y="250" width="100" height="40" fill="#ff9800" stroke="#333" />
                        <text x="500" y="275" text-anchor="middle" font-family="Arial" font-size="14" fill="white">Test</text>
                        
                        <!-- Labels -->
                        <text x="25" y="75" text-anchor="end" font-family="Arial" font-size="14">Fold 1</text>
                        <text x="25" y="125" text-anchor="end" font-family="Arial" font-size="14">Fold 2</text>
                        <text x="25" y="175" text-anchor="end" font-family="Arial" font-size="14">Fold 3</text>
                        <text x="25" y="225" text-anchor="end" font-family="Arial" font-size="14">Fold 4</text>
                        <text x="25" y="275" text-anchor="end" font-family="Arial" font-size="14">Fold 5</text>
                    </svg>
                    <p class="caption">5-Fold Cross-Validation</p>
                </div>
                
                <h3>Variations of Cross-Validation</h3>
                <ul>
                    <li><strong>Stratified K-Fold:</strong> Ensures that each fold maintains the same class distribution as the original dataset. Useful for imbalanced datasets.</li>
                    <li><strong>Leave-One-Out Cross-Validation (LOOCV):</strong> A special case of K-fold where K equals the number of samples. Each sample serves as a validation set once.</li>
                    <li><strong>Time Series Cross-Validation:</strong> Respects the temporal order of data, using earlier time periods for training and later periods for validation.</li>
                    <li><strong>Group K-Fold:</strong> Ensures that the same group (e.g., patients from the same hospital) is not in both training and validation sets.</li>
                </ul>
                
                <h3>Benefits of Cross-Validation</h3>
                <ul>
                    <li>Provides a more reliable estimate of model performance</li>
                    <li>Helps detect overfitting</li>
                    <li>Makes better use of limited data</li>
                    <li>Reduces the impact of data sampling on model evaluation</li>
                </ul>
                
                <div class="warning">
                    <strong>Common Mistakes:</strong> Performing feature selection or preprocessing on the entire dataset before cross-validation can lead to data leakage. Always include preprocessing steps within the cross-validation loop.
                </div>
                
                <h3>Code Example: K-Fold Cross-Validation</h3>
                <pre><code> 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Create models to evaluate
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(probability=True, random_state=42)
}

# Create pipelines with preprocessing
pipelines = {}
for name, model in models.items():
    pipelines[name] = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

# Perform stratified k-fold cross-validation
results = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for name, pipeline in pipelines.items():
    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')
    results[name] = scores
    print(f"{name}: {scores.mean():.4f} (+/- {scores.std():.4f})")

# Plot results
plt.figure(figsize=(10, 6))

box = plt.boxplot([results[name] for name in models.keys()], 
                patch_artist=True, 
                labels=list(models.keys()))

# Set box colors
colors = ['#2c7be5', '#6ea8fe', '#4d8be8']
for patch, color in zip(box['boxes'], colors):
    patch.set_facecolor(color)

plt.title('Model Comparison with 5-Fold Cross-Validation')
plt.ylabel('Accuracy')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()</code></pre>
            </section>
            
            <section id="model-selection" class="card">
                <h2>Model Selection</h2>
                
                <p>Model selection is the process of choosing the best model from a set of candidate models based on their performance on validation data.</p>
                
                <h3>Hyperparameter Tuning</h3>
                <p>Hyperparameters are parameters that are not learned from the data but set before training. Tuning these parameters can significantly improve model performance.</p>
                
                <h4>Common Hyperparameter Tuning Methods:</h4>
                <ul>
                    <li><strong>Grid Search:</strong> Exhaustively searches through a specified parameter grid</li>
                    <li><strong>Random Search:</strong> Randomly samples from the parameter space</li>
                    <li><strong>Bayesian Optimization:</strong> Uses probabilistic models to guide the search</li>
                </ul>
                
                <div class="note">
                    <strong>Important:</strong> Always perform hyperparameter tuning using cross-validation on the training set, not the test set. The test set should only be used for final evaluation.
                </div>
                
                <h3>Nested Cross-Validation</h3>
                <p>Nested cross-validation uses an inner loop for model selection (hyperparameter tuning) and an outer loop for model evaluation, providing an unbiased estimate of the true model performance.</p>
                
                <div class="image-container">
                    <svg width="500" height="300" xmlns="http://www.w3.org/2000/svg">
                        <!-- Outer fold 1 -->
                        <rect x="50" y="50" width="100" height="40" fill="#ff9800" stroke="#333" />
                        <rect x="150" y="50" width="300" height="40" fill="#2c7be5" stroke="#333" />
                        <text x="100" y="75" text-anchor="middle" font-family="Arial" font-size="14" fill="white">Test</text>
                        <text x="300" y="75" text-anchor="middle" font-family="Arial" font-size="14" fill="white">Train</text>
                        
                        <!-- Inner folds -->
                        <rect x="150" y="100" width="60" height="30" fill="#6ea8fe" stroke="#333" />
                        <rect x="210" y="100" width="240" height="30" fill="#4d8be8" stroke="#333" />
                        
                        <rect x="150" y="130" width="60" height="30" fill="#4d8be8" stroke="#333" />
                        <rect x="210" y="130" width="60" height="30" fill="#6ea8fe" stroke="#333" />
                        <rect x="270" y="130" width="180" height="30" fill="#4d8be8" stroke="#333" />
                        
                        <rect x="150" y="160" width="120" height="30" fill="#4d8be8" stroke="#333" />
                        <rect x="270" y="160" width="60" height="30" fill="#6ea8fe" stroke="#333" />
                        <rect x="330" y="160" width="120" height="30" fill="#4d8be8" stroke="#333" />
                        
                        <!-- Labels -->
                        <text x="25" y="75" text-anchor="end" font-family="Arial" font-size="14">Outer CV</text>
                        <text x="25" y="145" text-anchor="end" font-family="Arial" font-size="14">Inner CV</text>
                        
                        <text x="180" y="115" text-anchor="middle" font-family="Arial" font-size="12" fill="white">Val</text>
                        <text x="330" y="115" text-anchor="middle" font-family="Arial" font-size="12" fill="white">Train</text>
                        
                        <text x="240" y="145" text-anchor="middle" font-family="Arial" font-size="12" fill="white">Val</text>
                        
                        <text x="300" y="175" text-anchor="middle" font-family="Arial" font-size="12" fill="white">Val</text>
                    </svg>
                    <p class="caption">Nested Cross-Validation</p>
                </div>
                
                <h3>Model Selection Criteria</h3>
                <p>When selecting models, consider:</p>
                <ul>
                    <li><strong>Performance Metrics:</strong> Choose metrics aligned with your problem (accuracy, F1, AUC, etc.)</li>
                    <li><strong>Model Complexity:</strong> Simpler models are often preferred when performance is similar</li>
                    <li><strong>Inference Time:</strong> Consider computational requirements for deployment</li>
                    <li><strong>Interpretability:</strong> Some applications require explainable models</li>
                </ul>
                
                <h3>Code Example: Grid Search with Cross-Validation</h3>
                <pre><code>
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestClassifier(random_state=42))
])

# Define parameter grid
param_grid = {
    'model__n_estimators': [50, 100, 200],
    'model__max_depth': [None, 10, 20, 30],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 4]
}

# Perform grid search
grid_search = GridSearchCV(
    pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1
)

grid_search.fit(X_train, y_train)

# Print results
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")
print(f"Test set score: {grid_search.score(X_test, y_test):.4f}")

# Plot parameter importance
results = grid_search.cv_results_

# Extract parameter values and mean test scores
param_names = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']
param_values = {}
for param in param_names:
    param_values[param] = sorted(set([params[f'model__{param}'] for params in results['params']]))

# Plot the effect of each parameter
plt.figure(figsize=(15, 10))

for i, param in enumerate(param_names):
    plt.subplot(2, 2, i+1)
    
    # Group by parameter value and calculate mean score
    scores = []
    for value in param_values[param]:
        indices = [j for j, params in enumerate(results['params']) 
                if params[f'model__{param}'] == value]
        mean_score = np.mean([results['mean_test_score'][j] for j in indices])
        scores.append(mean_score)
    
    plt.plot(param_values[param], scores, 'o-', label=param)
    plt.xlabel(param)
    plt.ylabel('Mean CV Score')
    plt.title(f'Effect of {param}')
    plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()</code></pre>
            </section>
            
            <section id="practical-examples" class="card">
                <h2>Practical Examples</h2>
                
                <div class="tabs">
                    <div class="tab active" onclick="openTab(event, 'binary-classification')">Binary Classification</div>
                    <div class="tab" onclick="openTab(event, 'multi-class')">Multi-class Classification</div>
                    <div class="tab" onclick="openTab(event, 'regression-example')">Regression</div>
                </div>
                
                <div id="binary-classification" class="tab-content active">
                    <h3>Binary Classification Example</h3>
                    <p>This example demonstrates a complete evaluation workflow for a binary classification problem.</p>
                    
                    <pre><code>
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve
from sklearn.pipeline import Pipeline

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target
feature_names = data.feature_names

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')
print(f"Cross-validation accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# Train the final model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)
y_prob = pipeline.predict_proba(X_test)[:, 1]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Malignant', 'Benign'], 
            yticklabels=['Malignant', 'Benign'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='green', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(alpha=0.3)
plt.show()

# Feature Importance
feature_importances = pipeline.named_steps['classifier'].feature_importances_

# Sort feature importances in descending order
indices = np.argsort(feature_importances)[::-1]

# Plot the feature importances of the forest
plt.figure(figsize=(12, 8))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), feature_importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()</code></pre>
                </div>
                
                <div id="multi-class" class="tab-content">
                    <h3>Multi-class Classification Example</h3>
                    <p>This example demonstrates evaluation techniques for multi-class classification problems.</p>
                    
                    <pre><code>
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.multiclass import OneVsRestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target
class_names = iris.target_names

# Binarize the output for ROC curve
y_bin = label_binarize(y, classes=[0, 1, 2])
n_classes = y_bin.shape[1]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_bin, test_size=0.3, random_state=42)

# Create and train the model
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', SVC(kernel='rbf', probability=True, random_state=42))
])

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')
print(f"Cross-validation accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# Train the final model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names, 
            yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=class_names))

# ROC Curve for One-vs-Rest
classifier = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=42))
classifier.fit(X_train_bin, y_train_bin)

# Compute ROC curve and ROC area for each class
y_score = classifier.predict_proba(X_test_bin)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(10, 8))
colors = cycle(['blue', 'red', 'green'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
            label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

# Decision Boundary Visualization (for 2D projection)
from sklearn.decomposition import PCA

# Use PCA to reduce to 2 dimensions for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Train a new model on the PCA-transformed data
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
    X_pca, y, test_size=0.3, random_state=42)

model_pca = SVC(kernel='rbf', probability=True, random_state=42)
model_pca.fit(X_train_pca, y_train_pca)

# Create a mesh grid to visualize decision boundaries
h = 0.02  # step size in the mesh
x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Predict on the mesh grid
Z = model_pca.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.figure(figsize=(10, 8))
plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)

# Plot the training points
for i, color in zip(range(n_classes), ['blue', 'red', 'green']):
    idx = np.where(y_train_pca == i)
    plt.scatter(X_train_pca[idx, 0], X_train_pca[idx, 1], c=color, label=class_names[i],
                cmap=plt.cm.coolwarm, edgecolor='k', s=60)

# Plot the test points
for i, color in zip(range(n_classes), ['blue', 'red', 'green']):
    idx = np.where(y_test_pca == i)
    plt.scatter(X_test_pca[idx, 0], X_test_pca[idx, 1], c=color, marker='x',
                alpha=0.6, cmap=plt.cm.coolwarm, s=60)

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Decision Boundary Visualization')
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
                </div>
                
                <div id="regression-example" class="tab-content">
                    <h3>Regression Example</h3>
                    <p>This example demonstrates evaluation techniques for regression problems.</p>
                    
                    <pre><code>
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# Load dataset
boston = load_boston()
X = boston.data
y = boston.target
feature_names = boston.feature_names

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the model
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))
])

# Cross-validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores_r2 = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='r2')
cv_scores_neg_mse = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')

print(f"Cross-validation RÂ²: {cv_scores_r2.mean():.4f} (+/- {cv_scores_r2.std():.4f})")
print(f"Cross-validation RMSE: {np.sqrt(-cv_scores_neg_mse.mean()):.4f}")

# Train the final model
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Calculate metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\nTest set metrics:")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ²: {r2:.4f}")

# Residual Plot
plt.figure(figsize=(10, 6))
residuals = y_test - y_pred
plt.scatter(y_pred, residuals)
plt.axhline(y=0, color='r', linestyle='-')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.grid(alpha=0.3)
plt.show()

# Actual vs Predicted Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted')
plt.grid(alpha=0.3)
plt.show()

# Distribution of Residuals
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.grid(alpha=0.3)
plt.show()

# Feature Importance
feature_importances = pipeline.named_steps['regressor'].feature_importances_

# Sort feature importances in descending order
indices = np.argsort(feature_importances)[::-1]

# Plot the feature importances
plt.figure(figsize=(12, 8))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), feature_importances[indices], align="center")
plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.tight_layout()
plt.show()</code></pre>
                </div>
            </section>
            
            <section id="common-pitfalls" class="card">
                <h2>Common Pitfalls in Model Evaluation</h2>
                
                <div class="warning">
                    <h3>Data Leakage</h3>
                    <p>Data leakage occurs when information from outside the training dataset is used to create the model. This leads to overly optimistic performance estimates and poor generalization.</p>
                    
                    <h4>Common forms of data leakage:</h4>
                    <ul>
                        <li><strong>Target leakage:</strong> Using features that wouldn't be available at prediction time</li>
                        <li><strong>Train-test contamination:</strong> Performing preprocessing on the entire dataset before splitting</li>
                        <li><strong>Temporal leakage:</strong> Using future data to predict past events</li>
                    </ul>
                    
                    <h4>How to prevent data leakage:</h4>
                    <ul>
                        <li>Always split your data before any preprocessing</li>
                        <li>Use pipelines to ensure preprocessing steps are only fitted on training data</li>
                        <li>Be careful with time-dependent data</li>
                        <li>Remove features that wouldn't be available at prediction time</li>
                    </ul>
                </div>
                
                <h3>Other Common Pitfalls</h3>
                <ul>
                    <li><strong>Using accuracy for imbalanced datasets:</strong> Can be misleading when class distributions are skewed</li>
                    <li><strong>Overfitting to the validation set:</strong> Making too many modeling decisions based on validation performance</li>
                    <li><strong>Ignoring confidence intervals:</strong> Not considering the uncertainty in performance estimates</li>
                    <li><strong>Cherry-picking metrics:</strong> Reporting only the metrics that show good performance</li>
                    <li><strong>Not considering the business context:</strong> Using metrics that don't align with the actual problem requirements</li>
                </ul>
                
                <div class="tip">
                    <strong>Best Practice:</strong> Always define your evaluation metrics before starting the modeling process, based on the specific requirements of your problem.
                </div>
            </section>
            
            <section id="best-practices" class="card">
                <h2>Best Practices</h2>
                
                <h3>Model Evaluation Checklist</h3>
                <ul>
                    <li>âœ… Define clear evaluation metrics aligned with business objectives</li>
                    <li>âœ… Split data properly (training, validation, test sets)</li>
                    <li>âœ… Use cross-validation for more reliable performance estimates</li>
                    <li>âœ… Ensure no data leakage between training and evaluation data</li>
                    <li>âœ… Consider class imbalance and choose appropriate metrics</li>
                    <li>âœ… Evaluate both model performance and computational requirements</li>
                    <li>âœ… Compare multiple models using the same evaluation framework</li>
                    <li>âœ… Analyze errors and understand model limitations</li>
                    <li>âœ… Test the model on real-world or out-of-sample data when possible</li>
                    <li>âœ… Document the evaluation process and results</li>
                </ul>
                
                <h3>Choosing the Right Metrics</h3>
                <table>
                    <tr>
                        <th>Problem Type</th>
                        <th>Recommended Metrics</th>
                        <th>When to Use</th>
                    </tr>
                    <tr>
                        <td>Binary Classification (Balanced)</td>
                        <td>Accuracy, AUC-ROC, F1 Score</td>
                        <td>When both classes are equally important</td>
                    </tr>
                    <tr>
                        <td>Binary Classification (Imbalanced)</td>
                        <td>Precision, Recall, F1 Score, AUC-PR</td>
                        <td>When one class is more important or rare</td>
                    </tr>
                    <tr>
                        <td>Multi-class Classification</td>
                        <td>Accuracy, Macro/Weighted F1, Confusion Matrix</td>
                        <td>When distinguishing between multiple categories</td>
                    </tr>
                    <tr>
                        <td>Regression</td>
                        <td>RMSE, MAE, RÂ²</td>
                        <td>When predicting continuous values</td>
                    </tr>
                    <tr>
                        <td>Ranking</td>
                        <td>NDCG, MAP, MRR</td>
                        <td>When ordering items by relevance</td>
                    </tr>
                </table>
                
                <h3>Visualizing Model Performance</h3>
                <p>Effective visualizations can provide deeper insights into model performance:</p>
                <ul>
                    <li><strong>Confusion Matrix:</strong> Visualize classification errors</li>
                    <li><strong>ROC and PR Curves:</strong> Evaluate classification performance across thresholds</li>
                    <li><strong>Learning Curves:</strong> Diagnose overfitting/underfitting</li>
                    <li><strong>Residual Plots:</strong> Identify patterns in regression errors</li>
                    <li><strong>Feature Importance:</strong> Understand which features drive predictions</li>
                </ul>
                
                <div class="image-container">
                    <svg width="600" height="300" xmlns="http://www.w3.org/2000/svg">
                        <!-- Axes -->
                        <line x1="50" y1="250" x2="550" y2="250" stroke="#333" stroke-width="2"/>
                        <line x1="50" y1="50" x2="50" y2="250" stroke="#333" stroke-width="2"/>
                        
                        <!-- X-axis label -->
                        <text x="300" y="280" text-anchor="middle" font-family="Arial" font-size="14">Training Examples</text>
                        
                        <!-- Y-axis label -->
                        <text x="20" y="150" text-anchor="middle" font-family="Arial" font-size="14" transform="rotate(-90, 20, 150)">Score</text>
                        
                        <!-- Training curve -->
                        <path d="M 50,200 Q 150,100 300,70 T 550,50" stroke="#2c7be5" stroke-width="3" fill="none"/>
                        <text x="500" y="70" font-family="Arial" font-size="12" fill="#2c7be5">Training Score</text>
                        
                        <!-- Validation curve -->
                        <path d="M 50,200 Q 150,150 300,130 T 550,120" stroke="#ff9800" stroke-width="3" fill="none"/>
                        <text x="500" y="140" font-family="Arial" font-size="12" fill="#ff9800">Validation Score</text>
                        
                        <!-- Annotations -->
                        <text x="150" y="180" font-family="Arial" font-size="12">High Bias</text>
                        <text x="400" y="100" font-family="Arial" font-size="12">High Variance</text>
                        <text x="300" y="200" font-family="Arial" font-size="12">Optimal Model Complexity</text>
                    </svg>
                    <p class="caption">Learning Curve: Diagnosing Bias and Variance</p>
                </div>
            </section>
            
            <section id="resources" class="card">
                <h2>Additional Resources</h2>
                
                <h3>Books</h3>
                <ul>
                    <li>"Evaluating Machine Learning Models" by Alice Zheng</li>
                    <li>"Applied Predictive Modeling" by Max Kuhn and Kjell Johnson</li>
                    <li>"Feature Engineering and Selection" by Max Kuhn and Kjell Johnson</li>
                </ul>
                
                <h3>Online Courses and Tutorials</h3>
                <ul>
                    <li>Coursera: "Machine Learning" by Andrew Ng (Week 6: Evaluation)</li>
                    <li>Fast.ai: "Practical Deep Learning for Coders"</li>
                    <li>Scikit-learn Documentation: Model Evaluation Section</li>
                </ul>
                
                <h3>Tools and Libraries</h3>
                <ul>
                    <li><strong>scikit-learn:</strong> Comprehensive tools for model evaluation</li>
                    <li><strong>MLflow:</strong> Platform for managing the ML lifecycle, including tracking experiments</li>
                    <li><strong>TensorBoard:</strong> Visualization toolkit for TensorFlow</li>
                    <li><strong>SHAP:</strong> Library for model interpretability and feature importance</li>
                </ul>
            </section>
        </main>
    </div>
    
    <script>
        // Theme Toggle
        const themeToggle = document.getElementById('themeToggle');
        themeToggle.addEventListener('click', () => {
            document.body.classList.toggle('dark-theme');
            localStorage.setItem('theme', document.body.classList.contains('dark-theme') ? 'dark' : 'light');
        });
        
        // Check for saved theme preference
        if (localStorage.getItem('theme') === 'dark') {
            document.body.classList.add('dark-theme');
        }
        
        // Collapsible sections
        const collapsibles = document.getElementsByClassName('collapsible');
        for (let i = 0; i < collapsibles.length; i++) {
            collapsibles[i].addEventListener('click', function() {
                this.classList.toggle('active');
                const content = this.nextElementSibling;
                if (content.style.maxHeight) {
                    content.style.maxHeight = null;
                } else {
                    content.style.maxHeight = content.scrollHeight + 'px';
                }
            });
        }
        
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
        
        // Tabs functionality
        function openTab(evt, tabName) {
            // Declare all variables
            var i, tabContent, tabLinks;

            // Get all elements with class="tab-content" and hide them
            tabContent = document.getElementsByClassName("tab-content");
            for (i = 0; i < tabContent.length; i++) {
                tabContent[i].style.display = "none";
                tabContent[i].classList.remove("active");
            }

            // Get all elements with class="tab" and remove the class "active"
            tabLinks = document.getElementsByClassName("tab");
            for (i = 0; i < tabLinks.length; i++) {
                tabLinks[i].className = tabLinks[i].className.replace(" active", "");
            }

            // Show the current tab, and add an "active" class to the button that opened the tab
            document.getElementById(tabName).style.display = "block";
            document.getElementById(tabName).classList.add("active");
            evt.currentTarget.className += " active";
        }
    </script>
</body>
